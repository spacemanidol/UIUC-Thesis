Based on the aforementioned experiments, we generate 8 variants of oBERTa, each with a different size and sparsity profile; details can be found in table \ref{tab:oberta-model-info}. Within this table, we report the impact on the model size as measured by the raw and compressed size of the ONNX \footnote{https://onnx.ai/} model file. Embeddings are unpruned and each layer is pruned to the target sparsity profile independent of the rest of the model. As a result, the overall sparsity profile may vary as modules in the network may not be able to reach exactly 90\% or 95\% sparsity. \\
Using these \textit {inference-optimized} models, we evaluate their \textit{sparse transfer} performance by finetuning these models on their target task using a fixed training regime and minor hyperparameter exploration. For each task, we train them for 10 epochs or 20 (10 of which are Quantization Aware Training), with the longer schedule being reserved for models which are being quantized.  \\
We evaluate performance on a benchmark of diverse NLP tasks ranging from question answering, sentiment analysis, document classification, token classification, and text classification. For question answering, we leverage the SQuAD v1.1 \cite{Rajpurkar2016SQuAD10} and SQuAD V2.0 \cite{Rajpurkar2018KnowWY} datasets. We leverage the SST-2 \cite{socher-etal-2013-recursive} dataset for sentiment analysis. For text classification, we use the Quora Duplicate Query Detection (QQP) \cite{sambitsekhar_2017} and the MNLI \cite{N18-1101} datasets. We leverage the IMDB \cite{maas-EtAl:2011:ACL-HLT2011} dataset for document classification and CONLL2003 \cite{tjong-kim-sang-de-meulder-2003-introduction} for token classification.\\ 
Looking at performance on question answering as shown in table \ref{tab:sparse-transfer-squad} and \ref{tab:sparse-transfer-squad2}.
\begin{table}[!ht]
    \centering
    \tiny
    \scalebox{0.8}{
    \begin{tabular}{|l|*3l|*3l|}
    \toprule
         &  \multicolumn{3}{l}{Sparse Transfer} & \multicolumn{3}{l}{Sparse Transfer With Quantization} \\ 
        \midrule
        model & F1 & Recovery & EM & F1 & Recovery & EM \\ \hline
        oBERTa\textsubscript{base} & 92.15 & 100.00\% & 85.78 & 93.18 & 101.11\% & 87.29 \\ \hline
        oBERTa\textsubscript{base} 90$\backslash$\% & 90.95 & 98.69\% & 84.42 & 89.46 & 97.08\% & 82.61 \\ \hline
        oBERTa\textsubscript{base} 95$\backslash$\% & 89.84 & 97.49\% & 83.08 & 89.23 & 96.83\% &  81.12\\ \hline
        oBERTa\textsubscript{MEDIUM} & 90.37 & 98.06\% & 83.84 & 83.77 & 90.91\% & 90.37 \\ \hline
        oBERTa\textsubscript{MEDIUM} 90$\backslash$\% & 89.26 & 96.86\% & 82.18 & 88.65 & 96.20\% & 81.88 \\ \hline
        oBERTa\textsubscript{SMALL} & 84.87 & 92.09\% & 76.55 & 84.82 & 92.05\% & 76.77 \\ \hline
        oBERTa\textsubscript{SMALL} 90$\backslash$\% & 84.66 & 91.87\% & 76.18 & 82.18 & 92.18\% & 74.21\\ 
        \bottomrule
    \end{tabular}}
   
    \caption{Sparse Transfer performance of the oBERTA family on the SQUAD V1.1 dataset. The sparse transfer was performed over 10 epochs and sparse transfer with quantization over 20. Recovery is based on the relative performance of the unpruned oBERTa\textsubscript{base}. }
    \label{tab:sparse-transfer-squad}
\end{table}
\begin{table}[!ht]
    \centering
    \begin{tiny}
    \scalebox{0.8}{
    \begin{tabular}{|l|*3l|*3l|}
    \toprule
         &  \multicolumn{3}{l}{Sparse Transfer} & \multicolumn{3}{l}{Sparse Transfer With Quantization} \\ 
        \midrule
         model & F1 & Recovery & EM & F1 & Recovery & EM \\ \hline
        oBERTa\textsubscript{base} & 82.77 & 100.00\% & 79.56 & 85.298 & 103.06\% & 82.347 \\ \hline
        oBERTa\textsubscript{base} 90$\backslash$\% & 81.33 & 98.26\% & 78.27 & 81.43 & 98.38\% & 78.92 \\ \hline
        oBERTa\textsubscript{base} 95$\backslash$\% & 77.98 & 94.22\% & 74.67 & 78.09 & 94.35\% & 74.82 \\ \hline
        oBERTa\textsubscript{MEDIUM} & 77.51 & 93.65\% & 74.25 & 78.137 & 94.41\% & 75.179 \\ \hline
        oBERTa\textsubscript{MEDIUM} 90$\backslash$\% & 76.64 & 92.60\% & 73.34 & 76.24 & 92.11\% & 73.51 \\ \hline
        oBERTa\textsubscript{SMALL} & 71.54 & 86.44\% & 67.93 & 71.591 & 86.50\% & 68.087 \\ \hline
        oBERTa\textsubscript{SMALL} 90$\backslash$\% & 70.79 & 85.53\% & 67.31 & 69.35 & 87.79\% & 65.21 \\ \hline
        \bottomrule
    \end{tabular}}
    \end{tiny}
    \caption{Sparse Transfer performance of the oBERTA family on the SQUAD V2.0 dataset. The sparse transfer was performed over 10 epochs, and sparse transfer with quantization over 20. Recovery is based on the relative performance of the unpruned oBERTa\textsubscript{base}. }
    \label{tab:sparse-transfer-squad2}
\end{table}
Moving to text classification on QQP and MNLI as shown in tables \ref{tab:sparse-transfer-qqp} and \ref{tab:sparse-transfer-MNLI}
\begin{table}[!ht]
    \centering
    \begin{tiny}
    \scalebox{0.70}{
    \begin{tabular}{|l|*3l|*3l|}
    \toprule
         &  \multicolumn{3}{l}{Sparse Transfer} & \multicolumn{3}{l}{Sparse Transfer With Quantization} \\ 
        \midrule
         model & Accuracy & Recovery &Accuracy(MM) & Accuracy & Recovery & Accuracy(MM) \\ \hline
        oBERTa\textsubscript{base} & 87.88\% & 100.00\% & 87.57\% & 88.06\% & 100.20\% & 88.01\% \\ \hline
        oBERTa\textsubscript{base} 90\% & 85.17\% & 96.91\% & 84.73\% & 85.09\% & 96.83\% & 84.76\% \\ \hline
        oBERTa\textsubscript{base} 95\% & 84.32\% & 95.95\% & 84.08\% & 83.73\% & 95.28\% & 83.83\% \\ \hline
        oBERTa\textsubscript{MEDIUM} & 85.29\% & 97.05\% & 85.17\% & 83.62\% & 95.15\% & 83.74\% \\ \hline
        oBERTa\textsubscript{MEDIUM} 90\% & 81.61\% & 92.87\% & 81.32\% & 82.37\% & 93.73\% & 81.79\% \\ \hline
        oBERTa\textsubscript{SMALL} & 80.80\% & 91.95\% & 81.55\% & 81.10\% & 92.29\% & 81.51\% \\ \hline
        oBERTa\textsubscript{SMALL} 90\% & 79.23\% & 90.15\% & 79.24\% & 79.14\% & 90.06\% & 79.42\% \\ \hline
        \bottomrule
    \end{tabular}}
    \end{tiny}
    \caption{Sparse Transfer performance of the oBERTA family on the MNLI dataset. Sparse transfer was performed over 10 epochs and sparse transfer with quantization over 20. Recovery is based on the relative performance of the unpruned oBERTa\textsubscript{base}. }
    \label{tab:sparse-transfer-MNLI}
\end{table}
\begin{table}[!ht]
    \centering
    \begin{tiny}
    \scalebox{0.6}{
    \begin{tabular}{|l|*4l|*4l|}
    \toprule
         &  \multicolumn{4}{l}{Sparse Transfer} & \multicolumn{4}{l}{Sparse Transfer With Quantization} \\ 
        \midrule
        model & Accuracy & Recovery & F1 & Combined & Accuracy & Recovery & F1 & Combined \\ \hline
        oBERTa\textsubscript{base} & 91.52\% & 100.00\% & 90.09\% & 88.66\% & 89.86\% & 98.18\% & 88.12\% & 86.73\% \\ \hline
        oBERTa\textsubscript{base} 90\% & 91.01\% & 99.44\% & 89.47\% & 87.92\% & 91.21\% & 99.66\% & 89.68\% & 88.16\% \\ \hline
        oBERTa\textsubscript{base} 95\% & 90.85\% & 99.26\% & 89.21\% & 87.58\% & 90.72\% & 99.12\% & 89.08\% & 0.87\% \\ \hline
        oBERTa\textsubscript{MEDIUM} & 91.35\% & 99.81\% & 89.90\% & 88.44\% & 91.33\% & 99.79\% & 89.80\% & 88.28\% \\ \hline
        oBERTa\textsubscript{MEDIUM} 90\% & 90.48\% & 98.86\% & 88.85\% & 87.21\% & 90.60\% & 99.00\% & 89.01\% & 87.42\% \\ \hline
        oBERTa\textsubscript{SMALL} & 90.72\% & 99.13\% & 89.21\% & 87.71\% & 89.74 & 98.06\% & 87.99 & 86.25 \\ \hline
        oBERTa\textsubscript{SMALL} 90\% & 89.74\% & 98.06\% & 87.99\% & 86.25\% & 89.73 & 98.04\% & 87.98 & 86.08\\
        \bottomrule
    \end{tabular}}
    \end{tiny}
    \caption{Sparse Transfer performance of the oBERTA family on the QQP dataset. The sparse transfer was performed over ten epochs, and sparse transfer with quantization over 20. Recovery is based on the relative performance of the unpruned oBERTa\textsubscript{base}.}
    \label{tab:sparse-transfer-qqp}
\end{table}
Shifting focus to document classification as shown in table \ref{tab:sparse-transfer-imdb} and sentiment analysis in \ref{tab:sparse-transfer-SST2}
\begin{table}[!ht]
    \centering
    \begin{tiny}
    \scalebox{0.95}{
    \begin{tabular}{|l|*2l|*2l|}
    \toprule
         &  \multicolumn{2}{l}{Sparse Transfer} & \multicolumn{2}{l}{Sparse Transfer With Quantization} \\ 
        \midrule
        model & Accuracy & Recovery & Accuracy & Recovery \\ \hline
        oBERTa\textsubscript{base} & 95.24\% & 100.00\% & 95.44\% & 100.21\% \\ \hline
        oBERTa\textsubscript{base} 90\% & 93.64\% & 98.32\% & 93.28 & 97.94\% \\ \hline
        oBERTa\textsubscript{base} 95\% & 93.48\% & 98.15\% & 92.80& 97.23\% \\ \hline
        oBERTa\textsubscript{MEDIUM} & 93.36\% & 98.03\% & 94.08 & 98.78\% \\ \hline
        oBERTa\textsubscript{MEDIUM} 90\% & 92.24\% & 96.85\% & 92.08& 96.69\% \\ \hline
        oBERTa\textsubscript{SMALL} & 93.04\% & 97.69\% & 92.52 & 97.15\% \\ \hline
        oBERTa\textsubscript{SMALL} 90\% & 91.60\% & 96.18\% & 91.28 & 95.84\% \\
        \bottomrule
    \end{tabular}}
    \end{tiny}
    \caption{Sparse Transfer performance of the oBERTA family on the IMDB dataset. The sparse transfer was performed over ten epochs, and sparse transfer with quantization over 20. Recovery is based on the relative performance of the unpruned oBERTa\textsubscript{base}. }
    \label{tab:sparse-transfer-imdb}
\end{table}
\begin{table}[!ht]
    \centering
    \begin{tiny}
    \scalebox{0.95}{
    \begin{tabular}{|l|*2l|*2l|}
    \toprule
         &  \multicolumn{2}{l}{Sparse Transfer} & \multicolumn{2}{l}{Sparse Transfer With Quantization} \\ 
        \midrule
        model & Accuracy & Recovery & Accuracy & Recovery \\ \hline
        oBERTa\textsubscript{base} & 94.60 & 100.00\% & 92.66 & 97.95\% \\ \hline
        oBERTa\textsubscript{base} 90\% & 92.78 & 98.08\% & 92.546 & 97.83\% \\ \hline
        oBERTa\textsubscript{base} 95\% & 91.51 & 96.74\% & 91.399 & 96.62\% \\ \hline
        oBERTa\textsubscript{MEDIUM} & 92.89 & 98.19\% & 91.06 & 96.26\% \\ \hline
        oBERTa\textsubscript{MEDIUM} 90\% & 88.76 & 93.83\% & 89.91 & 95.04\% \\ \hline
        oBERTa\textsubscript{SMALL} & 90.48 & 95.64\% & 91.28 & 96.49\% \\ \hline
        oBERTa\textsubscript{SMALL} 90\% & 89.34 & 94.44\% & 88.65 & 93.71\% \\ 
        \bottomrule
    \end{tabular}}
    \end{tiny}
    \caption{Sparse Transfer performance of the oBERTA family on the SST-2 dataset. The sparse transfer was performed over ten epochs, and sparse transfer with quantization over 20. Recovery is based on the relative performance of the unpruned oBERTa\textsubscript{base}. }
    \label{tab:sparse-transfer-SST2}
\end{table}
Finally, looking at performance on token classification as shown in table \ref{tab:sparse-transfer-conll2003} 
\begin{table}[!ht]
    \centering
    \begin{tiny}
    \scalebox{0.75}{
    \begin{tabular}{|l|*3l|*3l|}
    \toprule
         &  \multicolumn{3}{l}{Sparse Transfer} & \multicolumn{3}{l}{Sparse Transfer With Quantization} \\ 
        \midrule
        model & Accuracy & Recovery & F1 & Accuracy & Recovery & F1 \\ \hline
        oBERTa\textsubscript{base} & 99.26\% & 100.00\% & 95.51\% & 99.30\% & 100.05\% & 95.98\% \\ \hline
        oBERTa\textsubscript{base} 90\% & 99.11\% & 99.85\% & 94.98\% & 99.05\% & 99.79\% & 94.51\% \\ \hline
        oBERTa\textsubscript{base} 95\% & 98.89\% & 99.63\% & 93.32\% & 98.75\% & 99.48\% & 92.61\% \\ \hline
        oBERTa\textsubscript{MEDIUM} & 99.04\% & 99.77\% & 94.39\% & 99.18\% & 99.92\% & 95.15\% \\ \hline
        oBERTa\textsubscript{MEDIUM} 90\% & 98.79\% & 99.53\% & 93.31\% & 98.73\% & 99.46\% & 92.70\% \\ \hline
        oBERTa\textsubscript{SMALL} & 99.01\% & 99.75\% & 94.00\% & 98.98\% & 99.72\% & 94.13\% \\ \hline
        oBERTa\textsubscript{SMALL} 90\% & 98.47\% & 99.20\% & 91.13\% & 98.25\% & 98.98\% & 89.79\% \\ 
        \bottomrule
    \end{tabular}}
    \end{tiny}
    \caption{Sparse Transfer performance of the oBERTA family on the CONLL-2003 dataset. The sparse transfer was performed over ten epochs, and sparse transfer with quantization over 20. Recovery is based on the relative performance of the unpruned oBERTa\textsubscript{base}. }
    \label{tab:sparse-transfer-conll2003}
\end{table}

\subsection{Inference Benchmark}
To evaluate the performance of our inference-optimized models, we benchmark performance using the popular DeepSparse library version 1.3.2 \footnote{pip install deepsparse==1.3.2} and an Intel Xeon Gold 6238R Processor. Performance is measured using models that have been \textit{sparse-transferred} to the SQuAD v1.1 dataset and exported to a standard ONNX model format. Benchmarks are run on 4 and 24 cores and a sequence length of 384 with batch sizes of 1, 16, and 64. For each model, the benchmark is run for 60 seconds with a warm-up period of 10 seconds, and we report the throughput (items per second) and the mean, median, and standard deviation per item latency. 
\begin{table}[!ht]
    \centering
    \begin{tiny}
    \scalebox{0.85}{
    \begin{tabular}{|l|*3l|*3l|}
    \toprule
         &  \multicolumn{3}{l}{24 Cores} & \multicolumn{3}{l}{4 Cores} \\ 
        \midrule
        Model & BS 1 & BS 16 & BS 64 &  BS 1 & BS 16 & BS 64 \\
        \midrule
        oBERTa\textsubscript{base} & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\ \hline
        oBERTa\textsubscript{base} Quantized & 3.10 & 4.29 & 4.46 & 4.09 & 4.31 & 4.32 \\ \hline
        oBERTa\textsubscript{base} 90\% & 3.29 & 3.80 & 3.80 & 3.60 & 3.34 & 3.40 \\ \hline
        oBERTa\textsubscript{base} 90\% Quantized & 4.12 & 7.05 & 7.37 & 7.67 & 7.59 & 7.40 \\ \hline
        oBERTa\textsubscript{base} 95\% & 8.72 & 4.56 & 4.65 & 4.12 & 3.85 & 4.37 \\ \hline
        oBERTa\textsubscript{base} 95\% Quantized & 4.73 & 8.22 & 8.56 & 9.41 & 9.06 & 8.68 \\ \hline
        oBERTa\textsubscript{MEDIUM} & 1.96 & 1.99 & 1.99 & 1.96 & 1.99 & 2.02 \\ \hline
        oBERTa\textsubscript{MEDIUM} Quantized & 6.20 & 8.04 & 8.44 & 8.43 & 8.33 & 8.45 \\ \hline
        oBERTa\textsubscript{MEDIUM} 90\%   & 6.35 & 7.41 & 6.84 & 7.83 & 6.56 & 6.72 \\ \hline
        oBERTa\textsubscript{MEDIUM} 90\% Quantized  & 8.94 & 12.86 & 13.65 & 14.99 & 14.81 & 14.95 \\ \hline
        oBERTa\textsubscript{SMALL} & 3.89 & 3.96 & 3.99 & 3.95 & 3.97 & 4.03 \\ \hline
        oBERTa\textsubscript{SMALL} Quantized & 12.47 & 14.12 & 14.08 & 15.50 & 15.48 & 15.70 \\ \hline
        oBERTa\textsubscript{SMALL} 90\% & 12.22 & 14.40 & 14.67 & 14.05 & 14.19 & 14.13 \\ \hline
        oBERTa\textsubscript{SMALL} 90\% Quantized & 16.21 & 21.35 & 23.96 & 29.77 & 27.14 & 27.58 \\ \hline
        \bottomrule
    \end{tabular}}
    \end{tiny}
    \caption{Latency reduction of the oBERTa family concerning the unpruned oBERTa\textsubscript{base} as measured on 24 and 4 cores. Speedup is measured relative to the latency reduction in MS/batch, and BS refers to batch size.}
    \label{tab:inference-bench}
\end{table}
We present a set of summary statistics of relative speedup across batch sizes and inference server configurations as shown in table \ref{tab:inference-bench}. Full inference performance results can be found in the appendix. In analyzing performance, we can see that the introduction of quantization to a dense model delivers roughly a 4x speedup while quantization on sparse models is closer to 2x. With the introduction of sparsity, 90\% leads to slightly under 4x speedup, while 95\% leads to slightly over 4x. The impact of structural pruning is roughly equivalent to the size of the as a 6-layer model is two times faster than a 12-layer, and a 3-layer model is four times faster. Since these different compression forms are additive, we can see that a small (3-layer) 90\% quantized model performance is ~24x (4*4*2). Looking at the variation in a speedup by batch size and the number of cores, we can see that allocating more cores leads to a smaller gap in inference speedup, especially with small batches. From this, we extract that compression is significant when performing streaming inference (batch size 1) on smaller CPUs.  \\
Next, we go ahead and benchmark the oBERTa model performance against existing sparse-transfer models such as oBERT and PruneOFA using the models that have been published \footnote{Since the PruneBERT model is not available in the zoo, we extrapolate numbers using the performance of our oBERTa\textsubscript{base} pruned 90\% as both models feature 12 transformer encoders and 90\% sparsity.} in Neural Magic's Sparse-Zoo \footnote{https://sparsezoo.neuralmagic.com/}. We run these models using four cores and a batch size of 1 and compare their speedup (or slowdown) relative to their performance on the SQUAD v1.1 question-answering benchmark. Results can be found in table \ref{tab:inference-competitive-short} and full results in \ref{tab:inference-competitive-full}. Looking at the improvements in accuracy and inference throughput, we find the oBERTa models are 1.3 to 4 times better than models with approximately the same accuracy.  \\
\begin{table}[!ht]
    \centering
    \scalebox{0.5}{
    \begin{tabular}{|l|l|*2l|*2l|}
    \toprule
        \multicolumn{2}{l}{} &  \multicolumn{2}{l}{Vs. BERT\textsubscript{base}} & \multicolumn{2}{l}{Vs. BERT\textsubscript{large}} \\ \hline
        Model & F1 & Recovery & Speedup & Recovery & Speedup \\ \hline
        oBERTa\textsubscript{base} 90\% & 91.00 & 102.77\% & 3.57 & 100.44\% & 20.21 \\ \hline
        oBERT\textsubscript{large} 95\% Quantized & 90.21 & 101.87\% & 3.41 & 99.57\% & 19.31 \\ \hline
        \midrule
        prunedOFA\textsubscript{large} 90\% Quantized & 89.96 & 101.59\% & 2.38 & 99.29\% & 13.47 \\ \hline
        oBERTa\textsubscript{base} 90\% Quantized & 89.46 & 101.03\% & 7.62 & 98.74\% & 43.07 \\ \hline
        \midrule
        oBERTa\textsubscript{MEDIUM} 90\%   & 89.26 & 98.99\% & 7.78 & 96.75\% & 43.99 \\ \hline
        obert\textsubscript{base} 90\% Quantized & 88.00 & 99.38\% & 6.96 & 97.13\% & 39.37 \\ \hline
        \midrule
        oBERTa\textsubscript{SMALL} 90\% & 84.66 & 90.97\% & 13.95 & 88.91\% & 78.91 \\ \hline
        pruneBERT 90\% & 84.90 & 95.88\% & 3.57  & 93.71\% & 73.82 \\ \hline
        \bottomrule
    \end{tabular}}
    \caption{Speedups of the oBERTa-family compared to existing published sparse models compared to the performance of BERT\textsubscript{base} and BERT-large. Speedup measures the reduction in latency of MS/batch. oBERTa\textsubscript{base} 90\% exceeds the accuracy of oBERT\textsubscript{large} 95\% quantized despite being faster, oBERTa\textsubscript{base} 90\% quantized performs at the level of pruneOFA\textsubscript{large} 90\% Quantized despite being 3x faster, oBERTa\textsubscript{MEDIUM} 90\% can outperform oBERT\textsubscript{base} 90\% Quantized despite being 30\% faster, and oBERTa\textsubscript{SMALL} 90\% performs on par with pruneBERT 90\% despite being nearly four times faster. }
    \label{tab:inference-competitive-short}
\end{table}
Looking at the competitive results, we find that the oBERTa-* models can deliver significant gains in performance (F1) relative to speedups. The oBERTa\textsubscript{base}Pruned 90\% Quantized model achieves an undertaking that nearly matches pruneOFA-large 90\% Quantized while delivering nearly 13x faster inference. Similarly, the oBERTA\textsubscript{SMALL} 90\%  model provides similar accuracy to PruneBERT despite being over four times faster.