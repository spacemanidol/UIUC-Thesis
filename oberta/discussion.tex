\textbf{Sparse Models require higher learning rates} as shown in the tables in \ref{sec:sparse-transfer-learning-rate} sparse language models can be used as general-purpose contextual language models but require the use of a much higher learning rate. When using structurally pruned models like the 6-layer oBERTa\textsubscript{MEDIUM} and the 3-layer oBERTa\textsubscript{SMALL}, the optimal learning rate does not vary much within the same task despite the model size. With the introduction of sparsity, the learning rate needs to scale, usually by a factor of five or ten. We find this counterintuitive as the sparse models have fewer parameters to \textit{tune}, so we would expect them to prefer a much lower learning rate. We attribute this to the loss of expressivity in the network driven by its sparsity. Since the network has fewer degrees of freedom to optimize the points which can be optimized moves much more than those that can't. \\
\textbf{Larger models compress better} as shown by the gap between the sparse and dense models and the gap between models and their quantized counterparts. While 12-layer models can receive 90 or 95 \% sparsity and quantization with little to no loss in accuracy, the three and 6-layer models see a much bigger dip. This aligns with Li et al. 2020 \cite{Li2020TrainLT} in which they demonstrate that larger models are more robust to pruning and quantization. Empirically, this makes sense as the smaller models have \textit{fewer degrees of freedom}, and other portions of the network cannot counteract the reduction in expressivity caused by pruning and quantization.   \\
\textbf{Bigger Teachers are not always better} as shown in the table in \ref{sec:sparse-transfer-KD} the introduction of larger teachers does not always lead to improvements in accuracy. The impact is highly task and model dependent as some datasets like MNLI or QQP see the little impact in using larger teachers, yet datasets like SQUAD or SQUAD v2.0  see large impacts, which are even more pronounced when the student model is smaller. \\
\textbf{Frozen embeddings can help}, but not always. As shown by \ref{sec:sparse-transfer-freeze-embd} the impact of freezing the embeddings is highly task-specific and inconsistent across tasks or models. In question answering, freezing leads to 1-2 point movement for unpruned models and 5-7 points for pruned models. In other tasks like QQP and MNLI, the impact of frozen embeddings tends to be minor or none. 