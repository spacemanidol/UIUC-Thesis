\section{Background}
Computers and computational devices have long been improving how humans think and act. At first, using a computer required large rooms of specialized equipment and expert operators. Since then, decades of gradual improvements led to the proliferation of cell phones and virtual supercomputers, which reside in the pockets of billions of people around the globe. These ubiquitous computing devices have become the interface that has brought the world into a truly connected mesh interface. Sharing one's daily experiences with thousands of viewers worldwide with little or no delay is not only possible but as common as driving a car. \\
In the world of artificial intelligence and natural language processing, there have also been decades of continuous improvement, which have led to models which can do truly impressive feats. Decades ago, it seemed far-fetched that one could ask a system obscure questions and receive the correct answer. Not only can this be done, but questions can come in spoken language, and responses sound natural despite their technological origin. \\
While personal computing devices can empower incredible experiences, most processing does not happen on devices. Instead, most of the \textit{work} happens in large data centers where economies of scale allow for cost-effective and failure-resistant infrastructure. Using customized deployments, services assemble their experiences by customizing complex workflows and experiences given their customer needs and constraints. \\
While all data center/cloud workloads are growing, those leveraging large neural network models have seen explosive growth. Models have grown from millions of parameters \cite{Krizhevsky2012ImageNetCW} to billions of \cite{Brown2020LanguageMA} and will likely reach trillions \cite{Fedus2021SwitchTS}. Despite the eye-catching model size growth, the bigger driver of computing is the widespread adoption of these models. In the earlier part of the 2010s, few companies were training AI models, and even fewer were using them. In 2022 79\% of companies had at least 3 models in production, up 17\% from a year before \footnote{https://www2.deloitte.com/us/en/pages/consulting/articles/state-of-ai-2022.html} \footnote{https://www2.deloitte.com/us/en/pages/about-deloitte/articles/press-releases/deloitte-state-of-ai-fourth-edition-report.html}. \\
In natural language processing, major advances have been driven by the proliferation of large models. One of the major goals of natural language processing is to understand human language in all its intricacies and uniqueness. Word representations created by using large textual corpora like GLoVE \cite{Pennington2014GloVeGV} and Word2Vec \cite{Mikolov2013DistributedRO} allowed for major improvements in tasks like sentiment analysis \cite{Socher2013RecursiveDM} and question answering \cite{Rajpurkar2016SQuAD1Q} \cite{Seo2017BidirectionalAF}. These methods provided a simple but effective vector representation, allowing major improvements in language understanding and representations. The field then experienced an explosion of new models and techniques with the arrival of the attention mechanism and large-scale self-supervised pretraining. These two forces collided to create countless large language models such as BERT \cite{Devlin2019BERTPO}, GPT-2 \cite{Radford2019LanguageMA}, T5 \cite{Raffel2020ExploringTL}, PALM \cite{Bi2020PALMPA}, etc.
\section{Motivation}
The usage of large language models has driven debate about their shortcomings, such as the biases they encode \cite{Nadeem2021StereoSetMS} \cite{Bender2021OnTD}, the extent to which they \textit{understand} language \cite{Petroni2019LanguageMA} \cite{Jiang2020HowCW}, and their environmental impact \cite{Strubell2020EnergyAP} \cite{Strubell2019EnergyAP}. Despite these challenges, the deployment and usage of these models have been explosive, with thousands of companies deploying unique models optimized to their business needs. Language models are running in search engines like Bing \footnote{https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/}and Google \footnote{https://blog.google/products/search/search-language-understanding-bert/}, in intelligent assistants like Siri and Alexa \cite{FitzGerald2022AlexaTM} and in many specialized use cases all of which are running billions of inference sessions. \\
The scale of language model deployment has motivated tremendous research into improving the shortcomings. Minor model accuracy and efficiency improvements can lead to millions of dollars in cost savings and empower new usage scenarios that previously seemed impossible. The scale of impact, even minor improvements in inference costs, has led to the creation of specialized companies like Neural Magic, Modular AI, OctoML, and DECI, to mention a few. Collective, they have received hundreds of millions of dollars of investment \footnote{https://deci.ai/news/deci-raises-25m-accelerate-ai-productization/} \footnote{https://techcrunch.com/2021/11/01/octoml-raises-85m-for-it-for-its-machine-learning-acceleration-platform/} \footnote{https://neuralmagic.com/blog/neural-magic-series-a/} to explore and commercialize improvements in model inference efficiency. \\ 
Large models' accuracy and ability to tackle diverse tasks without specialized understanding have led to treating models like large opaque boxes. Unlike the interpretable and inspectable decision trees they commonly replace, neural models provide little insight into predictions. As a result, large-scale deployments favor treating models as immutable units. 
\section{Thesis Contributions}
To address the challenges in deploying language models in web-scale workloads, this study studies approaches in compression and augmentation to improve robustness. The contributions of this work vary from highly applied experimental results to broad experiments and methodologies, all of which seek to inform on broadly applicable methods of making language models ready for large-scale production workloads. 
\subsection{Introducing and Transferring Sparsity For Efficient Inference}
Language models have favored scaling model size because performance improves with scale \cite{Hestness2017DeepLS} \cite{Hernandez2021ScalingLF} and large models are more sample efficient \cite{Kaplan2020ScalingLF}. This tendency to favor larger models often means that models are often over-parameterized and can greatly be compressed without large losses in accuracy. Compression methods vary in implementation and practice, but at their core, they seek to decrease the model size or computational cost without sacrificing a larger model's expressiveness. The Lottery Ticker Hypothesis, \cite{Frankle2019TheLT}, \cite{Chen2020TheLT} the finding that some or many sub-networks approximate the original network's performance has helped direct research into the introduction of structured and unstructured sparsity into models. By removing portions of the model, inference efficiency is greatly increased because the entire network is not needed to be executed simultaneously. \\
We explore how unstructured sparsity can be used for efficient inference in our work 
 \textit{The Optimal BERT Surgeon} \cite{Kurtic2022TheOB} finding that we are able to remove $90\%$ of network weights with little impact to model accuracy. We build on this work in \textit{Sparse*BERT} \cite{Campos2022SparseBERTSM} where we demonstrate that compressed models are to transfer to novel domains and tasks during pretraining without any specialized training or loss in accuracy. Combining unstructured sparsity and quantization with a sparsity-aware serving framework such as DeepSparse \footnote{https://github.com/neuralmagic/deepsparse} or TensorRT \footnote{https://developer.nvidia.com/tensorrt} model inference can be sped up over 15000\%. \footnote{https://neuralmagic.com/blog/obert/}.
\subsection{Accurate and Efficient Multi-Lingual Classification Workloads}
Traditional language models like BERT \cite{Devlin2019BERTPO} or BETO \cite{canete-etal-2022-albeto} tend to be monolingual and are focused on being used only for the language in which they were trained. While using machine translation as a processing step can provide effective predictions \cite{Isbister2021ShouldWS}, this approach requires additional inference and effective machine translation between all languages. Multi-Lingual language modeling seeks to avoid the difficulties of mass translation or many monolingual language models by simultaneously training a representation for many languages. This approach has widespread usage and is used for broad language agnostic question-answering classification and generation. \\
Using language models has become a natural part of the text-understanding toolkit for companies focusing on understanding customer insight. At Qualtrics, workloads exist for extracting insights from customer feedback, such as sentiment, emotion, topics, and actionability. While multi-lingual language models such as XLM-R \cite{Conneau2020UnsupervisedCR} allow classification workloads a simple and effective path to multi-linguality, their size can make web-scale deployments expensive and difficult. \\ 
In our work on Scaling up multi-lingual text classification on XM data with compressed language
models, \cite{Campos2022ScalM} we leverage quantization, multi-task learning, and knowledge distillation to improve model performance by 15x with minor losses in accuracy. Our experiments at Qualtrics on experience management data demonstrate how task-specific teachers are vital to delivering compressed models without sacrificing accuracy.
\subsection{Robust and Efficient Semantic Retrieval using Bi-Encoders}
Using bi-encoders and vector-based representations of documents and queries has led to major advances in information retrieval \cite{Karpukhin2020DensePR}. Leveraging language models as vector representation has led to effective and scalable semantic search, which can be applied to e-commerce \cite{Magnani2022SemanticRA} \cite{Bi2020ATE}, question answering \cite{Qu2021RocketQAAO}, and web search \cite{Xiong2021ApproximateNN}. \\
Despite the retrieval ability of bi-encoder usage in the real world can be difficult because of their sensitivity to typos and noise \cite{Sidiropoulos2022AnalysingTR} and inherited difficulty in scaling workload as they are based on compute-hungry transformers.  We study how bi-encoder models perform with noisy queries in our work \textit{CAPOT: Creating Robust Dense Query Encoders using Post Training Contrastive Alignment}. Contrastive Alignment Post Training, we can reduce accuracy losses on queries with typos by 55\% without model retraining nor index regeneration.  \\
Building on the simplicity of post-training modular optimization, our ongoing work in
\textit{Quick Dense Retrievers Consume KALE: Post Training Kullback–Leibler Alignment of Embeddings for Asymmetrical dual encoders} studies how bi-encoder asymmetry can be leveraged to improve model inference efficiency. Using KALE and less than 5 minutes on a consumer GPU leads to 4x faster inference with minimal losses in accuracy. 
\subsection{Scaling Sequence to Sequence Models to Web-Scale Workloads}
The use of sequence-to-sequence models has led to massive improvement in machine translation \cite{Vaswani2017AttentionIA}, abstractive summarization \cite{Zhang2020PEGASUSPW} and speech/audio transcription \cite{Radford2022RobustSR}. Part of the success of these models is driven by their ability to map an input to an output despite variability in length, type, or even domain. While effective, these models carry a high computational load as their architecture can be full of inefficiencies. While the encoder portion of the model runs once on the input, the decoder produces outputs iteratively until the end of the input tag is produced. As a result, usage can become bottle-necked when decoders produce long inputs, and performing batch processing results in non-optimal usage of computing as outputs have differing lengths. \\
Seeking to study how sequence-to-sequence models can be scaled to web-scale workloads, we seek to leverage the structural properties of these models to explore how T5 \cite{Raffel2020ExploringTL} and BART \cite{Lewis2020BARTDS} can be compressed to deliver maximal performance for multilingual abstractive summarization on datasets such as XSUM \cite{Narayan2018DontGM}, CNN/DailyMail \cite{Nallapati2016AbstractiveTS}. \\
\section{Document Structure Overview}
In chapter \ref{chp:lit}, we provide a literature review that covers language models, tasks and methods of using language models, methods of model compression, and shortcomings where language models can be brittle or produce unwanted outputs. In chapter \ref{chp:sparse}, we discuss some of the work we have completed around leveraging unstructured sparsity, knowledge distillation, and quantization to train, compress and transfer efficient inference models. In chapter \ref{chp:Multi}, we introduce and discuss our work improving the inference efficiency of multi-lingual text classification using quantization, structured pruning, multi-task modeling, and task-specific knowledge distillation. In chapter \ref{chp:search}, we discuss our work on improving the efficiency and robustness of bi-encoder-based retrieval using post-training alignment and compression. Finally, in chapter \ref{chp:encdec}, we discuss our future work on scaling generative workloads to web-scale deployments via asymmetrical compression. Further details on each chapter can be found below. 