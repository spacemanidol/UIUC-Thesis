The application of sequence-to-sequence language models has become an important tool for natural language processing tasks such as machine translation \cite{Sutskever2014SequenceTS}, audio transcription \cite{Radford2022RobustSR}, and abstractive summarization \cite{Raffel2020ExploringTL}. Sequence-to-sequence models effectively turn each of these aforementioned tasks into two-step problems: extraction and generation, and heavily condition the generation on the input. \\
Besides ensuring on-topic responses sequence to sequence models have the added benefit of being able to map inputs to targets with varying lengths and modalities in ways encoder or decoder-only systems cannot. \\
When used for abstractive summarization, sequence-to-sequence modeling has two steps, extraction using the encoder and generation using the decoder, which usually involves repeated execution until an end-of-sequence token is emitted. While the cost of encoder execution is essentially fixed on the batch size, the cost of decoder execution can be highly variable and difficult to predict. Despite the broad study of sequence-to-sequence models and how they compress research which studies the role of model symmetry as applied to inference efficiency and model accuracy is lacking. \\
Recent advances in scaling language models have led to a wide study on \textit{scaling laws} as applied to language model performance \cite{Kaplan2020ScalingLF}, training data size \cite{Hoffmann2022TrainingCL}, machine translation \cite{Henighan2020ScalingLF}, and even reinforcement learning \cite{Neumann2022ScalingLF}. \\
We build on this work and study the impact of scaling on abstractive summarization and what role model asymmetry has in it.
This asymmetry can manifest in various ways, such as the number of layers and hidden units in the encoder and decoder and the type of attention mechanisms used. \\
In this paper, we explore the role of asymmetry in the number of layers in encoder-decoder language modeling for summarization and its impact on the performance of these models. As shown in figure \ref{fig:speed}, the symmetry of pruning drives the impact on accuracy and inference speedups for sequence-to-sequence models. Pruning the encoder portion of the network leads to virtually no improvement in inference speed at the expense of accuracy. Pruning the decoder provides speedup with minor losses in accuracy. 
The following research questions drive our work: 
\begin{itemize}
    \item What scaling laws can be observed in abstractive summarization?
    \item What impact does encoder-decoder asymmetry have on abstractive summarization accuracy? 
    \item What impact does encoder-decoder asymmetry have on abstractive summarization inference efficiency?
    \item What is asymmetries impact on accuracy and inference efficiency does scale have in encoder-decoder models for abstractive summarization? 
\end{itemize}
It is in answering these questions that we deliver the following contributions: 
\begin{itemize}
\item We present the first robust study on scaling laws applied to the compression of sequence-to-sequence modeling. 
\item We demonstrate that the asymmetric inference cost of sequence-to-sequence models leads to asymmetric pruning for optimal inference efficient compression.
\item We empirically demonstrate on a wide variety of benchmarks how Asymmetric Compression can lead to a 2.7x inference speedup with no loss in accuracy on the XSUM dataset.
\end{itemize}