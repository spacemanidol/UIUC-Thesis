\textbf{Transformer Based Language Models} such as BERT \cite{Devlin2019BERTPO} and T5 \cite{Raffel2020ExploringTL} provide contextual language representations built on the Transformer architecture \cite{Vaswani2017AttentionIA} which can be specialized and adapted for specific tasks and domains \cite{Lee2020BioBERTAP}. Using these models, it becomes relatively easy to excel at a broad range of natural language processing tasks such as question answering, text classification, and sentiment analysis. \\
\begin{table*}[htb!]
    \centering
    \caption{Information about the architecture and attributes of the FLAN-T5 models}
    \scalebox{0.62}{
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
    \hline
        Model & Size(MBs) & Parameters & Encoder Layers & Parameters Encoder & Decoder Layers & Parameters decoder & Ratio End:Dec & Hidden Size \\ \hline
        Flan-t5-small \footnote{https://huggingface.co/google/flan-t5-small} & 146 & 60511616 & 8 & 35332800 & 8 & 41628352 & 0.849 & 512 \\ \hline
        Flan-t5-base \footnote{https://huggingface.co/google/flan-t5-base} & 472 & 222903552 & 12 & 109628544 & 12 & 137949312 & 0.795 & 768 \\ \hline
        Flan-t5-large \footnote{https://huggingface.co/google/flan-t5-large} & 1500 & 750251008 & 24 & 341231104 & 24 & 441918976 & 0.772 & 1024 \\ \hline
    \end{tabular}}
    \label{tab:models}
\end{table*}
\textbf{Scaling Laws} has become an increasingly important area of study as models' size and training data grows. Performance of the transformer-based language model improves with the relation to model size \cite{Radford2018ImprovingLU} and that larger models outperform smaller models \cite{Brown2020LanguageMA} on most NLP tasks. Increasing the training corpus size can lead to large improvements in performance, and model sizes can have a \textit{optimal} training data size \cite{Hoffmann2022TrainingCL}. Li et al. (2020) \cite{Li2020TrainLT} explore the relationship between model size and training efficiency finding larger models train faster and are more robust to pruning and quantization \cite{Na2022TrainFT}. \\
Rosenfeld et al. 2020 demonstrate that unstructured pruning impacts follow scaling laws \cite{Rosenfeld2020OnTP} where larger models can be pruned with greater ease. Despite this broad study of scaling laws, to our knowledge, we have not found any research focusing on the scaling laws of sequence-to-sequence models for summarization tasks. \\
\textbf{Asymmetrical in sequence-to-sequence models} broadly refers to non-uniformity between encoder and decoder model shape or attributes. Training and inference procedures should match as closely as possible \cite{Ranzato2015SequenceLT} \cite{Mihaylova2019ScheduledSF} as improvements in training loss during optimization result in improvements in model performance during Inference. While this may lead to the best model performance, it ignores the variable inference cost of models sequence to sequence models.  \\
During Inference, latency is dominated by the asymmetric execution of the language model. The auto-encoding encoder executes once over the entire input sequence, while the auto-regressive decoder executes iteratively until an end-of-sequence token is produced. \\
Kasai et al. demonstrated how the sequence-to-sequence language model performance for machine translation is dominated by the encoder depth \cite{Kasai2020DeepES}. Tay et al. 2021 extend this work by finding a \textit{DeepNarrow} which shows that for broad language modeling, it is possible to have 50\% fewer parameters and a 40\% faster inference with no loss in accuracy \cite{Tay2021ScaleEI}. \\
\textbf{Efficient Inference} for language modeling is a growing area of study that broadly focuses on reducing the inference cost without losses in accuracy. \\
Unstructured Pruning has been broadly studied \cite{Han2015ADN}  \cite{Sanh2020MovementPA} \cite{Kurti2022TheOB} \cite{Zafrir2021PruneOF} \cite{Campos2022SparseBERTSM} but realizing speedups can be difficult. \\ Structured Pruning removes fundamental structural components in a language model such as individual attention heads \cite{Voita2019AnalyzingMS} or entire model layers such as transformer encoders \cite{sanh2019distilbert}. \\
\textbf{Compressing Sequence-to-sequence} is a growing area of study where approaches from regular, efficient Inference has shown some transfer ability. Shleifer et al. show that it is possible to gain 1.93x speedup on a BART summarization model by applying structural pruning \cite{Shleifer2020PretrainedSD} but find compression approaches differ in their success depending on the dataset. Leveraging semi-structured pruning, Lagunas et al. can gain a 1.19 speedup \cite{Lagunas2021BlockPF} for minor losses in accuracy. While they find that the encoder is easier to prune than the decoder, they do not use this evidence of asymmetry to speed up performance further. \\
Li et al. investigate how to enable quantization, finding that without specialized distillation during quantization, performance collapses \cite{Li2022DQBARTES}.
Leveraging that generation occurs iteratively, and some tokens are easier to generate than other CALM \cite{Schuster2022ConfidentAL} apply early exiting to improve inference speed by 1.4x. While existing work has found interest in asymmetry, it has not been studied directly, nor has relationships in model scale been explored. \\
While there are other approaches such as knowledge distillation \cite{Hinton2015DistillingTK} \cite{sanh2019distilbert} \cite{Jiao2020TinyBERTDB}, quantization \cite{Zafrir2019Q8BERTQ8}, early exiting \cite{Xin2020DeeBERTDE} and token pruning \cite{Kim2021LearnedTP} these are not the focus on our work. 