\section{Overview}
In this chapter, we provide a broad overview of the relevant subject which we will discuss. First, we discuss language models, their importance, and their limitations. Next, we discuss the broad field of model compression and some popular approaches for model compression. Third, we discuss the application of language models as applied to information retrieval. 
\section{Language Models}
\subsection{What is language modeling and why is it useful}
Language modeling is a method of assigning a probability distribution over some form of language, like input. When applied to tokens in spoken or written human language modeling models, the probability of a token $w_i$ given the previous $i$ tokens as shown in equation \fullref{equation:langmodel}:
\begin{equation}
    P(w_{1},\ldots ,w_{m})=\prod _{i=1}^{m}P(w_{i}\mid w_{1},\ldots ,w_{i-1})\approx \prod _{i=1}^{m}P(w_{i}\mid w_{i-(n-1)},\ldots ,w_{i-1})
\label{equation:langmodel}
\end{equation} Language models can be useful methods to represent natural language because they allow models to differentiate meanings of sentences based on context. In other words, a model can understand that the word `fly' can mean different things in the sentences: `You look fly,' `Let's fly away!', `That is a fly. \\
While language modeling is by no means a new concept, it was not until the introduction of Neural Network-based LM that these representations could serve as general understanding frameworks. Before these Neural Network Language Models (NNLM), most language modeling usually focused on modeling some form of an N-gram where the probability of a word only depends on the previous $N$-word. Large Neural-Network-based LMs are the first step in an NLP application as a way of turning some form of textual input into a representation in a vector space. \\
Language models are created using many training objectives, but general models tend to be either auto-encoding (AE), auto-regressive (AR), or some combination. AR models like Elmo \cite{Peters2018DeepCW} or GPT-2 \cite{Radford2019LanguageMA} learn an LM by predicting the next token in a sequence. AE models like BERT \ mention {Devlin2019BERTPO} and ELECTRA \cite{Clark2020ELECTRAPT} learn an LM by reconstructing some sequence portion.
\subsection{Transformers}
Language Models are commonly built using multiple transformer layers to capture long-term input dependencies using self-attention \cite{Vaswani2017AttentionIA}. Each transformer usually has some variation of two sub-components: \textit{multi head attention (MHA)} and \textit{fully connected feed-forward networks (FFN)}. MHA contains many self-attention heads, each of which has three sub-components: queries (\textbf{Q}), keys (\textbf{K}), and values (\textbf{V}). The output of the attention component is the concatenation of each attention head and is fed into the FFN. 
The attention of each \textit{head} in MHA is formulated as:
\begin{equation}
 Attention(Q,K,V) = \textit{softmax} \left(\frac{QK}{\sqrt{\textit{d}}}\right)V,
 \end{equation}
\noindent where \textit{d} is the dimensionality of \textbf{K} and is used as a scaling parameter. 
The FFN is a fully-connected feed-forward network with linear transformations and an activation function such as ReLU or GeLU. 
\subsection{BERT}
Building on the success of ELMo, leveraging the transformer architecture \cite{Vaswani2017AttentionIA}, and taking the learning from other contextual word embeddings \cite{Howard2018UniversalLM} \cite{Radford2018ImprovingLU} Devlin et al., 2018 introduced BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is an AE LM that uses modified stacked Transformer encoders (12 layers for a small model and 24 for a large model) to build a contextual language representation. Instead of using character-level convolutions or fixed word vectors as a starting point, BERT leverages a piecewise tokenization \cite{Wu2016GooglesNM}, which sets a vocabulary size of 30,000.  \\
Like other language models before, BERT trains using unsupervised pre-training on a large text corpus. Unlike previous models, BERT introduces two new training objectives to steer the model: Masked Language Modeling (MLM) and next sentence prediction (NSP). \\
MLM reformulates language understanding as a cloze task \cite{Taylor1953ClozePA}, where the model's goal is to predict what a hidden word in a sentence may be. To train using MLM BERT introduces a new token $[MASK]$ to represent the hidden word. 15\% of each the corpus tokens are selected to be replaced of which 80\% (12\% of the corpus) is replaced with $[MASK]$, 10\%( 1.5\% of the corpus) is replaced with a random token, and the remaining 10\% are left alone. When the model finds a $[MASK]$ token, it predicts the word. NSP is a training method inspired by QA systems, which tend to have two sentences to reason on a query and a context passage. In NSP, the model is fed text, which combines two sentences, A and B, with the unique separation token [SEP]. In 50\% of the NSP samples, sentence B directly follows A, while in the remaining 50\%, A and B are selected randomly. The model has a binary training goal if the sentences are next to each other in the original text.\\
\subsection{Beyond BERT}
Besides BERT and Elmo, there has been considerable research into additional language models. RoBERTa \cite{Liu2019RoBERTaAR} improves on BERT by training on a larger corpus for a longer time. XLNET \cite{Yang2019XLNetGA} combines AE and AR while avoiding some of the pitfalls of each method by modifying AR to maximize the expected log-likelihood of a sequence concerning all permutations of factorization order. XLNET also removes the notion of a $[MASK]$ token to avoid training the model with a token that never occurs in text and implements the whole architecture using the Transformer-XL \cite{Dai2019TransformerXLAL}. ALBERT \cite{Lan2019ALBERTAL} explores the role of size in LM, finding that parameter weights can be shared across layers meaning they can have 18 times fewer parameters and train 1.7x faster than regular BERT all while producing similar language representation to BERT. DistilBERT \cite{Sanh2019DistilBERTAD} creates a smaller LM using knowledge distillation resulting in a similar performance to BERT with a 40\% smaller model. GPT \cite{Radford2018ImprovingLU}, GPT-2 \cite{Radford2019LanguageMA}, and GPT-3 \cite{Brown2020LanguageMA}  build an AR LM more suited toward language generation by using progressively larger models and a modified transformer decoder architecture. ELECTRA \cite{Clark2020ELECTRAPT} produces a model with comparable performance to BERT with substantially shorter training by having the model predict all tokens in a sentence instead of the $[MASK]$ token and by corrupting the input using a Generator similar to that of a GAN. Beyond these few models, we mention countless other optimizations and applications of this large-scale NNLM. 
\subsection{Relation To Thesis}
Language models have become cornerstones of most approaches to understanding language. Driven by this, in this thesis, we mainly study BERT, X-LMR, and T5—their broad usage and popularity cause our focus on these models. While more modern models have improved accuracy and efficiency, these models are not used nearly as often. As a result, our work focuses on improving performance for the most often used models. 
\section{Model Compression}
Given the ability to classify, predict and generate insights, AI models have become a large and common form of the computational workload. While these models can generate impressive results, using them at scale can be expensive and difficult and commonly require specialized inference infrastructures such as GPUs or FPGAs \cite{Yu2021ASO}. The high cost of using models has driven research to explore how to decrease the model size without losing accuracy. While many successful compression approaches have been pioneered outside of NLP\cite{Han2015ADN}\cite{LeCun1989OptimalBD} \cite{Han2016DeepCC}, Transformer models are  fragile~\cite{DBLP:journals/corr/abs-2105-06990}, as minor perturbations can lead to model collapse. Compression approaches usually start from a set of trained parameters $\theta$, e.g., a Transformer-based language model, and aim to produce a different model $\theta^*$ which approximates the accuracy of $\theta$ w.r.t. a given task-dependent loss function $\mathcal{L}$ while minimizing its cost $c$, but at lower parameter count and increased inference efficiency. In this formulation, model compression becomes a optimization of $\displaystyle{\minimize ((\mathcal{L}_{\theta}-\mathcal{L}_{\theta*})+ c(\theta*))}$ . Models are compressed by reducing the size or computational complexity of execution so that their performance mirrors or approximates that of the original model \cite{Molchanov2017PruningCN}.
\subsection{Iterative Compression}
Compression schemes are often motivated by weight saliency metrics which minimize the loss in accuracy due to the failure in the expressivity of the network. While there has been some success in compressing models without retraining \cite{Chen2021OnlyTO} \cite{Miao2022LearningPN} or using a single compression step \cite{Lee2019SNIPSN}, it is more common to compress models in a gradual iterative fashion. In this paradigm, a network is compressed in stages. Compression is applied at each compression step, where some portion of the network is selected for reduction, and the network is further trained to recover its complete accuracy. 
\subsection{Pruning}
\noindent\textbf{Pruning} is a set of compression approaches that decrease the model size and improve execution cost by removing portions of the network \cite{LeCun1989OptimalBD}. \\
\textbf{Unstructured} pruning removes individual neurons by setting them to zero \cite{Han2015ADN}, which can be exploited for storage and computational speedup. Unstructured Pruning seeks to find a proxy of importance to identify what portions of the network can be removed with the smallest impact on accuracy \cite{LeCun1989OptimalBD}. Zeroth order methods such as magnitude pruning \cite{Han2015ADN, Gale2019TheSO} assume weights as a proxy for importance and the smallest weight. First-order methods such as movement pruning \cite{Sanh2020MovementPA} estimate importance by measuring the movement of weights once they are transferred to a new task where weights that do not move are considered less important. Second-order \cite{LeCun1989OptimalBD, hassibi1993second, Singh2020WoodFisherES} methods such as Optimal Brain Surgeon (OBS) leverage complex Hessian approximations to determine the impact which Pruning may have. Other work has focused on creating pruned networks by identifying sparse \textit{lottery tickets} networks which transfer well to downstream tasks and approximate the uncompressed model \cite{Chen2020TheLT, Frankle2019TheLT}. In the realm of transformer-based LLMs, it has been shown that models can be compressed during pre-training so that there is little to no loss in accuracy when fine-tuned \cite{zafrir2021prune}. While unstructured Pruning can lead to massive compression in model parameters, improving inference speeds requires specialized hardware or sparsity-aware inference engines in practice. \\
\textbf{Structured} pruning \cite{LeCun1989OptimalBD} removes entire structural portions of a network, which in the scope of transformer-based language models layer in the encoder/decoder, decreasing the hidden size of smaller structures like attention heads. Structured pruning approaches require a structural understanding of the model to be successful. For transformer-based language models, the attention heads vary in importance, and nearly 40\% of heads can be removed without major impact on accuracy\cite{Michel2019AreSH,Voita2019AnalyzingMS}. Existing research has focused on evaluating how many transformer layers can be removed ~\cite{Sridhar2020UndividedAA} and the order in which they can be removed \cite{DBLP:journals/corr/abs-2004-03844}. Models like BORT \cite{DBLP:journals/corr/abs-2010-10499} combine structured Pruning with an optimization approach to produce smaller models designed around theoretically optimal sizes.\\

\textbf{Semi-structured} pruning is an intermediate approach where portions of the model are removed with a small, consistent grouping, such as a rectangular weight grouping \cite{lagunas21block} by setting their weights to zero. This approach is set to zero. This approach has recently gained popularity thanks to efficient computational support in GPUs leading to wide-scale, measurable inference improvements.
\subsection{Knowledge Distillation}
\textbf{Knowledge Distillation (KD)} \cite{Hinton2015DistillingTK} is an approach in which a compressed \textit{student} model is trained not to match the outputs of the dataset but the outputs of a larger and more accurate \textit{teacher} model by adding a loss component which minimizes the Kullback–Leibler divergence between the two output distributions as shown in Equation \ref{eq:kdloss}. KD is a form of label softening as a traditional target is a \textit{hard} one-hot vector representing the correct class. The learned outputs represent the candidate label distribution of a well-trained model.
uses a hardness parameter to control the mixture of regular loss and distillation loss and a temperature parameter to control the softness of the probability distribution.\\
\begin{equation}
    \mathcal{L}= h \mathcal{L}_d + (1-h) \mathcal{L}_{\ell}. 
\label{eq:kdloss}
\end{equation}
 \begin{equation}
     \mathcal{L}_d= {\displaystyle D_{\textit{KL}}(\theta^* \parallel \theta^\textit{t})=\sum _{x\in {\mathcal {X}}}P(x)\log \left({\frac {P(x)}{Q(x)}}\right).}
 \label{eq:kl}
 \end{equation}
KD has been broadly applied to transformer-based language models leading to general-purpose compressed models like DistillBERT \cite{Sanh2019DistilBERTAD}, while TinyBERT \cite{Jiao2020TinyBERTDB}, MobileBERT~\cite{Sun2020MobileBERTAC}, and MiniLM \cite{Wang2020MiniLMDS} have the student approximate the teacher's intermediate representations, obtaining better results at the cost of higher complexity. 
\subsection{Quantization}
Quantization decreases the cost of inference and model size by lowering the precision of weight and activation within a model \cite{Courbariaux2016BinarizedNN}. Most networks have their weights represented using \textit{int32}. Quantization methods produce $\theta^*$ by representing weights using less precise data structures such as \textit{int16} and \textit{int8}. While quantizing without retraining the model is possible, minor rounding errors can lead to significant losses in accuracy. As a result, performing  Quantization-Aware Training (QAT) is a common approach to ensure minimal loss in accuracy in compressed models. Specifically, QAT simulates the rounding effects on floating point values and leverages a Straight-Through Estimator (STE) to approximate the gradient, as quantization operations are not differentiable. While it is possible to produce networks that use one, two, or three bits for weight representation, lack of operator support can make it challenging to realize speedups, so most quantization focuses on int8 and int4. Q8BERT \cite{Zafrir2019Q8BERTQ8} can apply QAT and improved training regimes to produce a simple and extensible quantized language model. At the same time, TernaryBERT~\cite{zhang2020ternarybert} uses a complex distillation-based approach to obtain highly low-bit representations. In contrast, Fan et al. '20 \cite{fan2020training} propose a scheme that randomly quantizes group weights during training, leading to more accurate compressed models.
\subsection{Relation To Thesis}
This thesis primarily leverages unstructured pruning, structured pruning, quantization, and knowledge distillation. Using these approaches, we seek to improve model inference speeds, focusing on using commodity-grade CPUs. 
\section{Neural Methods For Information Retrieval}
The field of Information Retrieval has long studied how best to retrieve the most relevant information given constraints in corpora, inference needs, and many task-specific constraints. While term-based methods have long driven retrieval, the growth of datasets and language models has seen surging popularity both in research and real-world deployments.  
\subsection{Bi-Encoders}
\textbf{Bi-Encoders}, commonly called dual-encoders or dense retrievers, decompose ranking by leveraging the inner product of query and document representations to produce a relevance score for query document pairs. As their name suggests, bi-encoders leverage two encoders that run independently, one for the query and one for the passage. They are commonly called dense retrievers because of the density of their representation vectors compared to the sparsity of term-based retrieval methods. Dense retrievers can leverage contextual word representations without massive computational overhead by producing query and document representations. Since their document representations are query invariant, they can be pre-computed and loaded into an Approximate Nearest Neighbor (ANN) such as FAISS \cite{johnson2019billion}. At run time, the $k$ closest documents can be found for each query with minimal latency. Bi-encoders leverage LLM such as BERT \cite{Devlin2019BERTPO} for their text representation. As a result, they are often limited to ranking short passages of text and are commonly referred to as Dense Passage Retrievers (DPR) \cite{Karpukhin2020DensePR} \cite{Reimers2019SentenceBERTSE}. Driven by their efficiency in deployment and relevance performance, DPR-based models have rapidly become the building blocks for systems doing product search \cite{Magnani2022SemanticRA}, open domain question answering \cite{Karpukhin2020DensePR} and customer support \cite{Mesquita2022DenseTR}. \\
\subsection{Cross-Encoders}
Cross-Encoders are an application of language models which generate a rank or re-rank documents by producing a relevance score by scoring each possible query document pair. This method formulates the task as a binary classification where the query and candidate document into one text input, and a language model will predict whether this pair is relevant. A Cross-Encoder does not produce a sentence embedding and, as a result, can be much more computationally expensive when compared to a bi-encoder. While less efficient, Cross-Encoder performs better than Bi-Encoders and is more robust to noise or domain shift. \cite{Thakur2021BEIRAH}. As a result
\subsection{Training Methods}
\textbf{Data Augmentation} (DA) is a popular approach for improving how well models perform on new or noisy data. In data augmentation, training is extended by augmenting the training data with modifications or perturbations which match the desired model behavior. DA is extremely common in computer vision where training data is commonly rotated, blurred, cropped, or zoomed-in/out \cite{Mikoajczyk2018DataAF} \cite{Zhong2020RandomED}. DA has become increasingly more popular in NLP and has been used to improve model performance \cite{Jiao2020TinyBERTDB}, simulate large-scale training data when it is not available \cite{Li2020ADD}, and mitigate bias \cite{Lu2020GenderBI} in existing datasets. A detailed survey on DA approaches for NLP has been complied by Feng et al. 21' \cite{Feng2021ASO}.\\
\textbf{Contrastive Learning} builds on the notion of a contrastive loss \cite{Chopra2005LearningAS}, which seeks to create clusters in the embedding space such that examples with a shared class are far from other classes but close to each other. Much like learning that queries with noise have a shared intent, Schroff et al. 15' leverage contrastive learning to recognize faces despite different angles and perspectives \cite{Schroff2015FaceNetAU} by using a triplet loss. This approach is a natural fit for the world of search as relevance is at its core clustering relevant items close together and far from irrelevant items. Recently, contrastive learning has become a method for learning relevance at the corpora scale \cite{Xiong2021ApproximateNN} and improving DPR on noisy queries, \cite{Sidiropoulos2022AnalysingTR}.
\subsection{Compressing Bi-encoders}
The widespread utility of bi-encoders has demonstrated the need for compression. 
Seeking to improve the size of vector-based indexes has led to binary compression of dense representations \cite{Yamada2021EfficientPR} and broad experimentation about reducing the end-to-end size of retrieval systems \cite{Min2020NeurIPS2E}. While effective in the domain, these approaches have drawbacks as learned compression approaches can be brittle to domain shifts \cite{Thakur2022DomainAF}. Choi et al. '21 \cite{Choi2021ImprovingBD} show that using knowledge distillation with multiple teachers can maximize the performance of dense retrievers. 
\subsection{Relation To Thesis}
This thesis focuses on the bi-encoder models, which use language models to generate their vector representations. In subsequent chapters, we focus on finding and mitigating weaknesses in noisy inputs and inference efficiency. 
