\section{Overview}
In this chapter, we study how to develop efficient sequence-to-sequence models for generative tasks like abstractive summarization and audio transcription. 
\section{Introduction}
Language understanding models with an encoder-based architecture have been broadly employed because of their ability to produce contextual text representations which can be used to improve performance on tasks like question answering and text classification. The broad usage of these models has spurred broad and effective research to improve model inference efficiency and robustness to noise. While effective at language understanding, encoder-only models do not perform well in any generative or abstractive tasks. For generative tasks such as summarization, a sequence-to-sequence (seq2seq) approach which combines a transformer encoder and a transformer decoder such as T5 \cite{Raffel2020ExploringTL}, mT5 \cite{Xue2021mT5AM} and BART \cite{Lewis2020BARTDS} has emerged as a common architecture. This encoder-decoder structure has a unique property where it can map inputs and outputs with varying lengths and modalities without specialized processing or architecture. The ability to map structurally different inputs and outputs has shown tremendous success in abstractive summarization and machine translation where input and outputs differ in size, language, and vocabulary. When compared to single component structures seq2seq models can outperform both encoder-only \cite{Zhuang2022RankT5FT} and decoder-only \cite{Sanh2022MultitaskPT}. The seq2seq architecture is so effective that a well-trained and prompted T0 model can outperform a GPT-3 model on tasks such as  \cite{Sanh2022MultitaskPT} despite being over 15 times smaller. \\
Unlike uniform architectures, seq2seq architecture has been shown to be easily extensible to multi-modal domains with major architectural modifications. Radford et al. '22 \cite{Radford2022RobustSR} use a transformer-based seq2seq framework to perform audio transcription with high accuracy and efficiency. A transformer encoder is run on the outputs of an 80-channel log magnitude Mel audio spectrogram and joined with a text-only decoder to produce a model which can transcribe and translate audio with minimal errors. \\
\section{Motivation}
While seq2seq architecture has shown to be an effective approach to generating high-quality generative outputs, their size and unique methodology for execution can make them expensive to use for web-scale production tasks. While transformer-based encoder-only models have been the focus of compression research, there has been less research focusing on decoder-only models and almost no research into improving the efficiency of sequence-to-sequence models. Existing approaches to inference-efficient seq2seq modeling have cast the cost on inference as symmetric and thus have focused on equal compression of the encoder and decoder. Given that the inference cost for these models is asymmetrical, we believe we can improve inference efficiency by exploiting this asymmetry. \\
Training a sequence-to-sequence model commonly treats the two different components as one large model and trains in an end-to-end fashion. Since the model is trained jointly, the training cost is based on the model size. Using a sequence-to-sequence mode for inference is more nuanced due to the generative nature of the decoder output. While the encoder runs once on the entire input, the decoder generates its output one token at a time until an end-of-sequence tag is produced. Encoders, when employed as stand-alone units or in sequence-to-sequence models, have an inference cost of  $N*c(E)$ where $N$ represents the number of inputs and $c(E)$ is the cost of running inference on the encoder $E$. Decoders have an inference cost of  $N*(c(D)*T)$  where $T$ is the length of output produced. This asymmetrical cost makes the decoding cost the dominating term in the cost of running inference on a seq2seq model, $N*((c(D)*T)* c(E))$. Estimating the inference cost and determining optimal utilization patterns becomes more difficult when effective decoding strategies are considered. Beam search is typically used to generate various candidate outputs with the most likely candidate becoming the \textif{result}. In beam search, some threshold for the number of beams is selected and at each step, the top K tokens are selected as the candidate beams. While this strategy is an effective method of improving the quality of generated output it introduces a huge overhead in the inference efficiency. When batch grows, the variability in the length of all candidates causes large losses in efficiency, up to 71\% \cite{Yang2020ASA}. \\
While there has been some research studying how to compress and quantize sequence-to-sequence models \cite{Li2022DQBARTES} none of this research has been focused on enabling large-scale generative workloads. Unlike existing approaches which focus on efficiency by compressing mode size or minimizing the inference speed of individual inputs, our research is focused on minimizing the cost of performing large-scale inference workloads on web-scale datasets. By focusing on the requirements for processing large datasets our research can focus on exploiting how the execution methodologies of such architectures can be leveraged.  
\section{Proposed Experimentation}
Our experimentation will focus on understanding how the seq2seq model can best be compressed by leveraging the asymmetry and sequential nature of inference.  We broadly study the impact of asymmetry by evaluating multiple models, tasks, domains, and modalities. Given the cost and lengthy iteration cycles associated with model pretraining, we will focus our work on improving model inference using existing models. Our research focuses on the area where sequence-to-sequence models have most broadly been used: abstractive text summarization and audio transcription. While there have been successful applications of this architecture in other domains we do not focus our research there. \\
In our experiments, we will leverage existing state-of-the-art modeling approaches and explore how compression can be optimized to minimize the associated cost of processing millions of documents. We will sequence to sequence models of varying sizes and then perform experiments compressing and aligning these models to explore the role of asymmetry and deriving optimal methods of performing large generation tasks. Our work will both be theoretical and applied as we will explore approaches for modeling a develop a framework for executing large-scale inference in a cost-effective way. 
\subsection{Research Questions}
Our experimentation in compression methodologies focuses on studying how asymmetrical approaches in model compression impact sequence-to-sequence modeling which we frame as the following research questions across a variety of modalities and tasks:
\begin{itemize}
\item What impact does asymmetry in sequence-to-sequence model depth have on model accuracy and inference efficiency?
\item Can encoders and decoders of different sizes and shapes are easily combined to maximize model performance given a latency bu
\item How do compression approaches impact model accuracy when applied asymmetrically?
\item How does beam size approach the efficiency and 
\item Does increasing the size of the extractor 
\end{itemize}
\subsection{Abstractive Text Summarization}
Abstractive text summarization, which is often also called generative summarization, is a task where the goal is to take some input sequence $X$ and produce an output $Y$ such that $Y$ provides a useful summary of $X$ which is faith full to the original text and includes proper grammar. Summarization is an example of context-driven generation as the goal is to produce $Y$ which includes information from $X$ but also common language attributes that may not actually be in $X$. Summarization models are generally useful for broad domains and are broadly deployed by search engines as a way of generating short text snippets for websites which can be used for further ranking or by the user as information that helps them decide to click or not. Our experiments will focus on some of the most popular summarization benchmarks such as X-Sum \cite{Narayan2018DontGM}, CNN/DailyMail \cite{Nallapati2016AbstractiveTS}, Reddit TIFU \cite{Kim2019AbstractiveSO}, and XL-SUM \cite{Hasan2021XLSumLM} and explore how the T5 \cite{Raffel2020ExploringTL} and  BART \cite{Lewis2020BARTDS} can be compressed to allow for efficient and effective web scale summarization workloads. Evaluation is done in a similar fashion for all the aforementioned datasets using the ROUGE-*, \cite{Lin2004ROUGEAP} scores comparing the generated summary to the candidate.\\
The CNN/Daily mail abstractive text summarization dataset is a collection of documents from CNN and Daily Mail that have been annotated for descriptive summaries. The dataset consists of 311,672 documents and their respective summaries. \\
The Reddit TIFU dataset is an abstractive text summarization collected from a portion of the online messaging community Reddit. It consists of 122,933 documents which have then been summarized by a human. The dataset consists of short and long summaries with an average length of 9 and 23 words respectively. \\
X-Sum is a dataset for training and evaluation of single-source document summarization. Given a document $X$ the goal is to produce a summary $Y$ which answers the question \textit{What is a document about?}. It is comprised of 226,711 articles along and their human-generated summary. The documents come from BBC articles covering broad topics like news, sports, and technology from 2010-2017. \\
XL-SUM is an extension of the X-SUM dataset in both scale and linguality covered. It features 1 million articles across 44 languages each of which has had a summary generated by a journalist professional.
\subsection{Audio Transcription}
Audio transcription is a joint audio and language task where the goal is to take a sound input $X$ and produce an output $Y$ which is a faithful transcription of what a speaker has said in $X$. s Research in audio transcription methods has long been a thriving field of research with many paradigms but one of the most popular and common methods of transcription consists of using some form of an encoder/feature extractor on the raw audio input followed by a decoder for transcript generation. \\
Our experiments will focus on comparing the impact of compression on seq2seq transcription models across common audio transcription datasets such as LibreSpeech \cite{Panayotov2015LibrispeechAA}, Multilingual LibriSpeech \cite{Pratap2020MLSAL}, FLEURS \cite{Conneau2022FLEURSFL}, and Common Voice \cite{Ardila2020CommonVA}. Each of these datasets features some form of audio and an associated correct transcription and models are evaluated by the amount of error their transcriptions have known as word error rate.  \\
The LibriSpeech dataset is a collection of 1,000 hours of transcription of audiobooks mostly derived from Project Gutenberg. This dataset is designed to test the efficiency of training with its various train partitions of varying size and development and test sets designed how well systems perform on transcription depending on the cleanliness of the text.\\
The Multilingual LibriSpeech is a large multilingual variant of the LibreSpeech dataset. It provides labeled transcription for audiobooks in English, German, Dutch, Spanish, French, Italian, Portuguese, and Polish providing 44.5K hours of labeled English data and 6K hours for other languages. \\
The FLEURS dataset is a parallel speech dataset in 102 languages which includes 12 hours of labeled data supervision per language. While it can be used for tasks like language identification, translation, and retrieval we will only leverage it for transcript generation. \\
The Common Voice is a dataset that includes transcriptions for 9283 hours of recorded conversations. What makes common voice unique is it features wide degrees of recording metadata such as age, sex, and accent which allows for a thorough evaluation of model robustness and bias. 
\section{Experimentation Plan}
\scalebox{1}{
\begin{tabular}{r |@{\foo} l}
December 2022 & Asymmetry and Structural Pruning of text generation systems \\
December 2022 & Methodology and related work write-ups \\
January 2023 & Asymmetry and Structural Pruning of audio transcription systems \\
January 2023 & Knowledge Distillation and Size of Teachers for text generation systems \\
February 2023 & Knowledge Distillation and Size of Teachers for audio transcription systems \\
February 2023 & Experimental write ups and discussions \\
March 2023 & Optimizing inference using compressed models benchmarking \\
\end{tabular}
}