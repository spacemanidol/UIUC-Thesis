@inproceedings{isonuma2017extractive,
  title     = {Extractive summarization using multi-task learning with document classification},
  author    = {Isonuma, Masaru and Fujino, Toru and Mori, Junichiro and Matsuo, Yutaka and Sakata, Ichiro},
  booktitle = {Proceedings of the 2017 Conference on empirical methods in natural language processing},
  pages     = {2101--2110},
  year      = {2017}
}

@inproceedings{Conneau2019UnsupervisedCR,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzm{\'a}n and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}
@article{he2019interactive,
  title   = {An interactive multi-task learning network for end-to-end aspect-based sentiment analysis},
  author  = {He, Ruidan and Lee, Wee Sun and Ng, Hwee Tou and Dahlmeier, Daniel},
  journal = {arXiv preprint arXiv:1906.06906},
  year    = {2019}
}

@inproceedings{vijayaraghavan2017twitter,
  title     = {Twitter demographic classification using deep multi-modal multi-task learning},
  author    = {Vijayaraghavan, Prashanth and Vosoughi, Soroush and Roy, Deb},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages     = {478--483},
  year      = {2017}
}

@inproceedings{singla2018multi,
  title     = {A multi-task approach to learning multilingual representations},
  author    = {Singla, Karan and Can, Do{\u{g}}an and Narayanan, Shrikanth},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages     = {214--220},
  year      = {2018}
}


@inproceedings{lin2018multi,
  title     = {A multi-lingual multi-task architecture for low-resource sequence labeling},
  author    = {Lin, Ying and Yang, Shengqi and Stoyanov, Veselin and Ji, Heng},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {799--809},
  year      = {2018}
}


@article{chen2018multi,
  title     = {Multi-task learning for dangerous object detection in autonomous driving},
  author    = {Chen, Yaran and Zhao, Dongbin and Lv, Le and Zhang, Qichao},
  journal   = {Information Sciences},
  volume    = {432},
  pages     = {559--571},
  year      = {2018},
  publisher = {Elsevier}
}


@inproceedings{pironkov2016multi,
  title     = {Multi-task learning for speech recognition: an overview.},
  author    = {Pironkov, Gueorgui and Dupont, Stephane and Dutoit, Thierry},
  booktitle = {ESANN},
  year      = {2016}
}


@article{liu2019mkd,
  title   = {Mkd: a multi-task knowledge distillation approach for pretrained language models},
  author  = {Liu, Linqing and Wang, Huan and Lin, Jimmy and Socher, Richard and Xiong, Caiming},
  journal = {arXiv preprint arXiv:1911.03588},
  year    = {2019}
}

@article{liu2019improving,
  title   = {Improving multi-task deep neural networks via knowledge distillation for natural language understanding},
  author  = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal = {arXiv preprint arXiv:1904.09482},
  year    = {2019}
}

@inproceedings{li2020knowledge,
  title     = {Knowledge Distillation for Multi-task Learning},
  author    = {Li, Wei-Hong and Bilen, Hakan},
  booktitle = {Computer Vision--ECCV 2020 Workshops: Glasgow, UK, August 23--28, 2020, Proceedings, Part VI},
  pages     = {163--176},
  year      = {2020}
}


@inproceedings{wang2018glue,
  title     = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author    = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle = {Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages     = {353--355},
  year      = {2018}
}

@article{smith2022using,
  title   = {Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author  = {Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal = {arXiv preprint arXiv:2201.11990},
  year    = {2022}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{wang2020minilm,
  title   = {Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author  = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal = {arXiv preprint arXiv:2002.10957},
  year    = {2020}
}

@article{wang2020minilmv2,
  title   = {MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers},
  author  = {Wang, Wenhui and Bao, Hangbo and Huang, Shaohan and Dong, Li and Wei, Furu},
  journal = {arXiv preprint arXiv:2012.15828},
  year    = {2020}
}

@article{sanh2019distilbert,
  title   = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author  = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal = {arXiv preprint arXiv:1910.01108},
  year    = {2019}
}

@inproceedings{jiao2020tinybert,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4163--4174},
  year={2020}
}

@inproceedings{zhao2021extremely,
  title={Extremely Small BERT Models from Mixed-Vocabulary Training},
  author={Zhao, Sanqiang and Gupta, Raghav and Song, Yang and Zhou, Denny},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={2753--2759},
  year={2021}
}

@inproceedings{mukherjee2020xtremedistil,
  title     = {XtremeDistil: Multi-stage Distillation for Massive Multilingual Models},
  author    = {Mukherjee, Subhabrata and Awadallah, Ahmed Hassan},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages     = {2221--2234},
  year      = {2020}
}

@article{mukherjee2021xtremedistiltransformers,
  title   = {XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation},
  author  = {Mukherjee, Subhabrata and Awadallah, Ahmed Hassan and Gao, Jianfeng},
  journal = {arXiv preprint arXiv:2106.04563},
  year    = {2021}
}

@article{jiao2021lightmbert,
  title   = {LightMBERT: A Simple Yet Effective Method for Multilingual BERT Distillation},
  author  = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal = {arXiv preprint arXiv:2103.06418},
  year    = {2021}
}

@inproceedings{gupta2020compression,
  title     = {Compression of Deep Learning Models for NLP},
  author    = {Gupta, Manish and Varma, Vasudeva and Damani, Sonam and Narahari, Kedhar Nath},
  booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages     = {3507--3508},
  year      = {2020}
}

@article{yang2019model,
  title={Model compression with multi-task knowledge distillation for web-scale question answering system},
  author={Yang, Ze and Shou, Linjun and Gong, Ming and Lin, Wutao and Jiang, Daxin},
  journal={arXiv preprint arXiv:1904.09636},
  year={2019}
}

@article{gou2021knowledge,
  title     = {Knowledge distillation: A survey},
  author    = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal   = {International Journal of Computer Vision},
  volume    = {129},
  number    = {6},
  pages     = {1789--1819},
  year      = {2021},
  publisher = {Springer}
}

@article{hinton2015distilling,
  title   = {Distilling the knowledge in a neural network},
  author  = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal = {arXiv preprint arXiv:1503.02531},
  year    = {2015}
}

@article{de2020optimal,
  title   = {Optimal subarchitecture extraction for bert},
  author  = {de Wynter, Adrian and Perry, Daniel J},
  journal = {arXiv preprint arXiv:2010.10499},
  year    = {2020}
}

@article{turc2019well,
  title   = {Well-read students learn better: On the importance of pre-training compact models},
  author  = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal = {arXiv preprint arXiv:1908.08962},
  year    = {2019}
}

@inproceedings{radosavovic2018data,
  title     = {Data distillation: Towards omni-supervised learning},
  author    = {Radosavovic, Ilija and Doll{\'a}r, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4119--4128},
  year      = {2018}
}

@inproceedings{clark2019bam,
  title     = {BAM! Born-Again Multi-Task Networks for Natural Language Understanding},
  author    = {Clark, Kevin and Luong, Minh-Thang and Khandelwal, Urvashi and Manning, Christopher D and Le, Quoc},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages     = {5931--5937},
  year      = {2019}
}

@inproceedings{tarvainen2017mean,
  title     = {Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author    = {Tarvainen, Antti and Valpola, Harri},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {1195--1204},
  year      = {2017}
}

@article{hou2020dynabert,
  title   = {Dynabert: Dynamic bert with adaptive width and depth},
  author  = {Hou, Lu and Huang, Zhiqi and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Liu, Qun},
  journal = {arXiv preprint arXiv:2004.04037},
  year    = {2020}
}

@article{xu2021bert,
  title   = {NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search},
  author  = {Xu, Jin and Tan, Xu and Luo, Renqian and Song, Kaitao and Li, Jian and Qin, Tao and Liu, Tie-Yan},
  journal = {arXiv preprint arXiv:2105.14444},
  year    = {2021}
}

@inproceedings{conneau2020unsupervised,
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  author    = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages     = {8440--8451},
  year      = {2020}
}

@article{transformerslifting,
  title  = {Lifting the Curse of Multilinguality by Pre-training Modular Transformers},
  author = {Transformers, Pre-training Modular}
}

@inproceedings{lauscher2020zero,
  title     = {From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers},
  author    = {Lauscher, Anne and Ravishankar, Vinit and Vuli{\'c}, Ivan and Glava{\v{s}}, Goran},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {4483--4499},
  year      = {2020}
}

@inproceedings{ponti2020xcopa,
  title     = {XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning},
  author    = {Ponti, Edoardo Maria and Glava{\v{s}}, Goran and Majewska, Olga and Liu, Qianchu and Vuli{\'c}, Ivan and Korhonen, Anna},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {2362--2376},
  year      = {2020}
}

@inproceedings{artetxe2017learning,
  title     = {Learning bilingual word embeddings with (almost) no bilingual data},
  author    = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {451--462},
  year      = {2017}
}

@inproceedings{pfeiffer2020mad,
  title     = {MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer},
  author    = {Pfeiffer, Jonas and Vuli{\'c}, Ivan and Gurevych, Iryna and Ruder, Sebastian},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {7654--7673},
  year      = {2020}
}

@inproceedings{pfeiffer2021adapterfusion,
  title     = {AdapterFusion: Non-Destructive Task Composition for Transfer Learning},
  author    = {Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages     = {487--503},
  year      = {2021}
}

@inproceedings{pfeiffer2020AdapterHub,
  title     = {AdapterHub: A Framework for Adapting Transformers},
  author    = {Jonas Pfeiffer and
               Andreas R\"uckl\'{e} and
               Clifton Poth and
               Aishwarya Kamath and
               Ivan Vuli\'{c} and
               Sebastian Ruder and
               Kyunghyun Cho and
               Iryna Gurevych},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020): Systems Demonstrations},
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.7},
  pages     = {46--54}
}

@article{ruckle2020adapterdrop,
  title   = {Adapterdrop: On the efficiency of adapters in transformers},
  author  = {R{\"u}ckl{\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna},
  journal = {arXiv preprint arXiv:2010.11918},
  year    = {2020}
}

@article{tay2020efficient,
  title   = {Efficient transformers: A survey},
  author  = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal = {arXiv preprint arXiv:2009.06732},
  year    = {2020}
}

@article{wei2021finetuned,
  title   = {Finetuned language models are zero-shot learners},
  author  = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal = {arXiv preprint arXiv:2109.01652},
  year    = {2021}
}

@inproceedings{pmlr-v119-kurtz20a,
  title     = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks},
  author    = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {5533--5543},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  address   = {Virtual},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},
  url       = {http://proceedings.mlr.press/v119/kurtz20a.html},
  abstract  = {Optimizing convolutional neural networks for fast inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network. Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear (ReLU) activation functions. In this paper, we present an in-depth analysis of methods for maximizing the sparsity of the activations in a trained neural network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains. To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit (FATReLU). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss. Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale ImageNet image classification task on modern Intel CPUs, with little or no retraining cost.}
}
@misc{singh2020woodfisher,
  title         = {WoodFisher: Efficient Second-Order Approximation for Neural Network Compression},
  author        = {Sidak Pal Singh and Dan Alistarh},
  year          = {2020},
  eprint        = {2004.14340},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@inproceedings{collobert2008unified,
  title     = {A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author    = {Collobert, Ronan and Weston, Jason},
  booktitle = {Proceedings of the 25th international conference on Machine learning},
  pages     = {160--167},
  year      = {2008}
}

@misc{roberta,
  doi       = {10.48550/ARXIV.1907.11692},
  url       = {https://arxiv.org/abs/1907.11692},
  author    = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{XLMR,
  doi       = {10.48550/ARXIV.1911.02116},
  url       = {https://arxiv.org/abs/1911.02116},
  author    = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{conneau2019cross,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{zafrir2019,
  author    = {Ofir Zafrir and
               Guy Boudoukh and
               Peter Izsak and
               Moshe Wasserblat},
  title     = {{Q8BERT:} Quantized 8Bit {BERT}},
  journal   = {CoRR},
  volume    = {abs/1910.06188},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.06188},
  eprinttype = {arXiv},
  eprint    = {1910.06188},
  timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-06188.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang2020metapoison,
  title={Metapoison: Practical general-purpose clean-label data poisoning},
  author={Huang, W Ronny and Geiping, Jonas and Fowl, Liam and Taylor, Gavin and Goldstein, Tom},
  booktitle={NeurIPS},
  year={2020}
}

@article{geffen2022distilprotbert,
  title={DistilProtBert: A distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts},
  author={Geffen, Yaron and Ofran, Yanay and Unger, Ron},
  journal={bioRxiv},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
 booktitle={EMNLP-IJCNLP},
  year={2019}
}

@inproceedings{liang2020mixkd,
  title={MixKD: Towards efficient distillation of large-scale language models},
  author={Liang, Kevin J and Hao, Weituo and Shen, Dinghan and Zhou, Yufan and Chen, Weizhu and Chen, Changyou and Carin, Lawrence},
  journal={arXiv preprint arXiv:2011.00593},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{wu2021one,
  title={One teacher is enough? pre-trained language model distillation from multiple teachers},
  author={Wu, Chuhan and Wu, Fangzhao and Huang, Yongfeng},
  booktitle={ACL-IJCNLP},
  year={2021}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{bai2020binarybert,
  author    = {Haoli Bai and
               Wei Zhang and
               Lu Hou and
               Lifeng Shang and
               Jing Jin and
               Xin Jiang and
               Qun Liu and
               Michael R. Lyu and
               Irwin King},
  title     = {BinaryBERT: Pushing the Limit of {BERT} Quantization},
  journal   = {CoRR},
  volume    = {abs/2012.15701},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.15701},
  eprinttype = {arXiv},
  eprint    = {2012.15701},
  timestamp = {Thu, 05 May 2022 16:29:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-15701.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bort,
  author    = {Adrian de Wynter and
               Daniel J. Perry},
  title     = {Optimal Subarchitecture Extraction For {BERT}},
  journal   = {CoRR},
  volume    = {abs/2010.10499},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.10499},
  eprinttype = {arXiv},
  eprint    = {2010.10499},
  timestamp = {Mon, 26 Oct 2020 15:39:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-10499.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{LeCun1989OptimalBD,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}