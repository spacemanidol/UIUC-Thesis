\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzmán, Grave, Ott, Zettlemoyer, and Stoyanov}]{XLMR}
Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.; Wenzek, G.; Guzmán, F.;
  Grave, E.; Ott, M.; Zettlemoyer, L.; and Stoyanov, V. 2019.
\newblock Unsupervised Cross-lingual Representation Learning at Scale.

\bibitem[{Conneau and Lample(2019)}]{conneau2019cross}
Conneau, A.; and Lample, G. 2019.
\newblock Cross-lingual language model pretraining.
\newblock \emph{Advances in neural information processing systems}, 32.

\bibitem[{de~Wynter and Perry(2020)}]{de2020optimal}
de~Wynter, A.; and Perry, D.~J. 2020.
\newblock Optimal subarchitecture extraction for bert.
\newblock \emph{arXiv preprint arXiv:2010.10499}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
\newblock {BERT}: Pre-training of Deep Bidirectional Transformers for Language
  Understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, 4171--4186. Minneapolis,
  Minnesota: Association for Computational Linguistics.

\bibitem[{He et~al.(2019)He, Lee, Ng, and Dahlmeier}]{he2019interactive}
He, R.; Lee, W.~S.; Ng, H.~T.; and Dahlmeier, D. 2019.
\newblock An interactive multi-task learning network for end-to-end
  aspect-based sentiment analysis.
\newblock \emph{arXiv preprint arXiv:1906.06906}.

\bibitem[{Hinton, Vinyals, and Dean(2015)}]{hinton2015distilling}
Hinton, G.; Vinyals, O.; and Dean, J. 2015.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}.

\bibitem[{Jiao et~al.(2020)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu}]{jiao2020tinybert}
Jiao, X.; Yin, Y.; Shang, L.; Jiang, X.; Chen, X.; Li, L.; Wang, F.; and Liu,
  Q. 2020.
\newblock TinyBERT: Distilling BERT for Natural Language Understanding.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, 4163--4174.

\bibitem[{Jiao et~al.(2021)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu}]{jiao2021lightmbert}
Jiao, X.; Yin, Y.; Shang, L.; Jiang, X.; Chen, X.; Li, L.; Wang, F.; and Liu,
  Q. 2021.
\newblock LightMBERT: A Simple Yet Effective Method for Multilingual BERT
  Distillation.
\newblock \emph{arXiv preprint arXiv:2103.06418}.

\bibitem[{Kurtz et~al.(2020)Kurtz, Kopinsky, Gelashvili, Matveev, Carr, Goin,
  Leiserson, Moore, Nell, Shavit, and Alistarh}]{pmlr-v119-kurtz20a}
Kurtz, M.; Kopinsky, J.; Gelashvili, R.; Matveev, A.; Carr, J.; Goin, M.;
  Leiserson, W.; Moore, S.; Nell, B.; Shavit, N.; and Alistarh, D. 2020.
\newblock Inducing and Exploiting Activation Sparsity for Fast Inference on
  Deep Neural Networks.
\newblock In III, H.~D.; and Singh, A., eds., \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, 5533--5543. Virtual: PMLR.

\bibitem[{LeCun, Denker, and Solla(1989)}]{LeCun1989OptimalBD}
LeCun, Y.; Denker, J.; and Solla, S. 1989.
\newblock Optimal brain damage.
\newblock \emph{Advances in neural information processing systems}, 2.

\bibitem[{Lin et~al.(2018)Lin, Yang, Stoyanov, and Ji}]{lin2018multi}
Lin, Y.; Yang, S.; Stoyanov, V.; and Ji, H. 2018.
\newblock A multi-lingual multi-task architecture for low-resource sequence
  labeling.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, 799--809.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{roberta}
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.;
  Zettlemoyer, L.; and Stoyanov, V. 2019.
\newblock RoBERTa: A Robustly Optimized BERT Pretraining Approach.

\bibitem[{Mukherjee and Awadallah(2020)}]{mukherjee2020xtremedistil}
Mukherjee, S.; and Awadallah, A.~H. 2020.
\newblock XtremeDistil: Multi-stage Distillation for Massive Multilingual
  Models.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, 2221--2234.

\bibitem[{Mukherjee, Awadallah, and
  Gao(2021)}]{mukherjee2021xtremedistiltransformers}
Mukherjee, S.; Awadallah, A.~H.; and Gao, J. 2021.
\newblock XtremeDistilTransformers: Task Transfer for Task-agnostic
  Distillation.
\newblock \emph{arXiv preprint arXiv:2106.04563}.

\bibitem[{Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}]{NEURIPS2019_9015}
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen,
  T.; Lin, Z.; Gimelshein, N.; Antiga, L.; Desmaison, A.; Kopf, A.; Yang, E.;
  DeVito, Z.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.;
  Bai, J.; and Chintala, S. 2019.
\newblock PyTorch: An Imperative Style, High-Performance Deep Learning Library.
\newblock In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d\textquotesingle
  Alch\'{e}-Buc, F.; Fox, E.; and Garnett, R., eds., \emph{Advances in Neural
  Information Processing Systems 32}, 8024--8035. Curran Associates, Inc.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf}]{sanh2019distilbert}
Sanh, V.; Debut, L.; Chaumond, J.; and Wolf, T. 2019.
\newblock DistilBERT, a distilled version of BERT: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}.

\bibitem[{Singh and Alistarh(2020)}]{singh2020woodfisher}
Singh, S.~P.; and Alistarh, D. 2020.
\newblock WoodFisher: Efficient Second-Order Approximation for Neural Network
  Compression.
\newblock arXiv:2004.14340.

\bibitem[{Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti et~al.}]{smith2022using}
Smith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhandari, S.; Casper, J.;
  Liu, Z.; Prabhumoye, S.; Zerveas, G.; Korthikanti, V.; et~al. 2022.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock \emph{arXiv preprint arXiv:2201.11990}.

\bibitem[{Vijayaraghavan, Vosoughi, and Roy(2017)}]{vijayaraghavan2017twitter}
Vijayaraghavan, P.; Vosoughi, S.; and Roy, D. 2017.
\newblock Twitter demographic classification using deep multi-modal multi-task
  learning.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, 478--483.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang2018glue}
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2018.
\newblock GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
  Language Understanding.
\newblock In \emph{Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
  Analyzing and Interpreting Neural Networks for NLP}, 353--355.

\bibitem[{Wang et~al.(2020{\natexlab{a}})Wang, Bao, Huang, Dong, and
  Wei}]{wang2020minilmv2}
Wang, W.; Bao, H.; Huang, S.; Dong, L.; and Wei, F. 2020{\natexlab{a}}.
\newblock MiniLMv2: Multi-Head Self-Attention Relation Distillation for
  Compressing Pretrained Transformers.
\newblock \emph{arXiv preprint arXiv:2012.15828}.

\bibitem[{Wang et~al.(2020{\natexlab{b}})Wang, Wei, Dong, Bao, Yang, and
  Zhou}]{wang2020minilm}
Wang, W.; Wei, F.; Dong, L.; Bao, H.; Yang, N.; and Zhou, M.
  2020{\natexlab{b}}.
\newblock Minilm: Deep self-attention distillation for task-agnostic
  compression of pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2002.10957}.

\bibitem[{Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz et~al.}]{wolf2019huggingface}
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.;
  Rault, T.; Louf, R.; Funtowicz, M.; et~al. 2019.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}.

\bibitem[{Yang et~al.(2019)Yang, Shou, Gong, Lin, and Jiang}]{yang2019model}
Yang, Z.; Shou, L.; Gong, M.; Lin, W.; and Jiang, D. 2019.
\newblock Model compression with multi-task knowledge distillation for
  web-scale question answering system.
\newblock \emph{arXiv preprint arXiv:1904.09636}.

\bibitem[{Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and
  Wasserblat}]{zafrir2019}
Zafrir, O.; Boudoukh, G.; Izsak, P.; and Wasserblat, M. 2019.
\newblock {Q8BERT:} Quantized 8Bit {BERT}.
\newblock \emph{CoRR}, abs/1910.06188.

\end{thebibliography}
