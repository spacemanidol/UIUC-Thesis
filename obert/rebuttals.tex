% ==> these are imports from their original emnlp2022.tex
\pdfoutput=1
\documentclass[11pt]{article}
%\usepackage[review]{EMNLP2022}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
% ==> ends here

%\usepackage{authblk} % <-- problematic
\usepackage{fullpage}
\usepackage{enumitem}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{booktabs}       % professional-quality tables
\usepackage{array}          % tables with fixed lengths - enables using "m" to center content of non-multirow cells
\usepackage{colortbl}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcommand{\greyrule}{\arrayrulecolor{black!30}\midrule\arrayrulecolor{black}}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}

%\renewcommand\Affilfont{\fontsize{9}{10}\itshape} <-- problematic
% ===== define new commands here =====
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\wm}{\vect{w}_M}
\newcommand{\ws}{\vect{w}^*}
\newcommand{\w}{\mathbf{w}}
\newcommand{\hess}{\vect{H_{\mathcal{L}}}}
\newcommand{\pr}[1]{\left(#1\right)}
\newcommand{\dw}{\delta \w}
\newcommand{\dL}{\delta \mathcal{L}}
\newcommand{\eF}{\widehat{\vect{F}}}
\newcommand{\x}{\vect{x}}
\newcommand{\e}{\vect{e}}
\newcommand{\E}{\vect{E}}
\newcommand{\subQ}{\textrm{Q}}
\newcommand{\tile}{\mathbf{\tilde{e}}}
\newcommand{\hessinv}{\mathbf{H}^{-1}}
\def\1{\bm{1}}
% ==================================

\newcommand{\delw}{\delta \mathbf{w}}

\newcommand{\comment}[1]{}
\newcommand{\mynote}[3]{
    \fbox{\bfseries\sffamily\scriptsize#1}
{\small$\blacktriangleright$\textsf{\emph{\color{#3}{#2}}}$\blacktriangleleft$}}
\newcommand{\eldar}[1]{\mynote{Eldar}{#1}{orange}}
\newcommand{\dan}[1]{\mynote{Dan}{#1}{blue}}

\newcommand{\bert}{$\textrm{BERT}_{\textrm{BASE}}\,$}
\linespread{0.98}

\pgfplotsset{width=8cm,compat=1.9}
\graphicspath{ {./media/} }

\begin{document}

\section{Pruning for GPU speedups (N:M sparsity pattern)}
\begin{table}[h!]
    \caption{Comparison between oBERT one-shot 2-out-of-4 (2:4) pruning and Magnitude Pruning. Methods such as Lottery-Ticket, Movement Pruning, Prune OFA, $l_0$ Regularization, and PLATON  require fine-tuning and therefore do not support one-shot pruning. Fine-tuning the oBERT 2:4 pruned model for only 1-epoch fully recovers dense model accuracy with (F1, EM) = (88.58, 81.16). }
    \centering
    {\small
    \begin{tabular}{cc|cc|c}
    \toprule 
    Task & \makecell{BERT-Base} & \makecell{Magnitude} & oBERT (ours) & GPU speedup \\
    \midrule
    \makecell{SQuAD \\ F1, EM} & 88.54, 81.41 & 49.97, 35.24 & \cellcolor{green!25}\textbf{83.17, 74.18} & 1.85x \\ 
    \bottomrule
    \end{tabular}
    }
\end{table}

\section{Additional models: unstructured pruning}

\begin{table}[h!]
    \caption{\textbf{Comparison between oBERT and the upstream SOTA Prune OFA method, when pruning the \mbox{BERT-Large model} at 90\% sparsity}. Even the model pruned with oBERT at double the sparsity (95\%) outperforms Prune OFA.}
    \centering
    {\small
    \begin{tabular}{ccc|cc}
    \toprule 
    Task & BERT-Large & Sparsity & Prune OFA & oBERT (ours) \\
    \midrule
    \makecell{SQuAD\\F1, EM} & 91.22, 84.45 & 90\% & 90.20, 83.35 & \cellcolor{green!25}\textbf{91.00, 84.50}\\
    \midrule
    \makecell{SQuAD\\F1, EM} & 91.22, 84.45 & 95\% & NA & 90.29, 83.58\\
    \bottomrule
    \end{tabular}
    }
\end{table}

\section{Additional models: compound compression for edge deployment}
\begin{table}[h!]
    \caption{\textbf{Compressing {BERT-Large} and \textbf{MobileBERT} models on the SQuADv1 task, with the goal of recovering >99\% of the dense BERT-Large accuracy}. oBERT-Large stands for our 95\% block4 pruned and quantized BERT-Large model, and oBERT-MobileBERT stands for a 14-layer, 50\% block4 pruned and quantized MobileBERT model. Both models are produced following the compound compression approach described in the paper. Models were evaluated with the DeepSparse inference engine, using a server with two Intel(R) Xeon(R) Platinum 8380 (IceLake) CPUs with 40 cores each, batch-size 128 and sequence length 384.}
    \centering
    {\small
    \begin{tabular}{ccccccccc}
    \toprule 
    Model & Precision & \makecell{F1 Score\\(R=X\% recovery)} & File Size & \makecell{Compression\\Ratio} & \makecell{Throughput\\(samples/sec)} & Speedup \\
    \midrule
    \makecell{BERT-Large\\dense baseline} & FP32 & 90.87 (R=100\%) & 1.30 GB & 1x & 15.49 & 1x \\
    \midrule
    oBERT-Large & INT8 & 90.21 (R=99.27\%) & 38.20 MB & \phantom{1}34x & 230.74 & 15x \\
    oBERT-MobileBERT & INT8 & 90.32 (R=99.39\%) & \phantom{1}9.56 MB & 136x & 928.58 & 60x \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\section{Additional GLUE results}
\begin{table}[h!]
    \caption{\textbf{Additional results on GLUE tasks where baselines from other work are available, complementing Table 2 in the submission}. 
    By contrast to competing work, oBERT results are obtained \textbf{without any per-task hyper-parameter tuning}. The same setup is used for all results presented in the paper (SQuAD, MNLI, QQP) and results shown here (SST-2, QNLI).}
    \label{tab:with_KD}
    \centering
    {\small
    \begin{tabular}{ccc|ccc}
    \toprule 
    Task & \makecell{BERT-Base} & Sparsity & LT-BERT & Prune OFA & oBERT (ours) \\
    \midrule
    \makecell{SST-2 \\ Accuracy} & 93.01 & 90\% & 85.00$^*$ & 90.88 & \cellcolor{green!25}\textbf{92.20} \\
    \midrule
    \makecell{QNLI \\ Accuracy} & 91.25 & 90\% & 80.00$^*$ & 89.07 & \cellcolor{green!25}\textbf{89.97} \\
    \bottomrule
    \end{tabular}
    }
\end{table}

\section{Additional GLUE results + comparison with \emph{concurrent} work}
\begin{table}[h!]
    \caption{
    \textbf{Comparison with concurrent work PLATON (ICML 2022)}. 
    For fair comparison, we remove Knowledge-Distillation (KD) during fine-tuning because the competing methods do not use it. 
    All oBERT results are obtained \textbf{without any per-task hyper-parameter tuning}, except for early stopping to prevent overfitting on tiny GLUE tasks. 
    By contrast, the competing PLATON work reports best results after extensive task-specific hyper-parameter search. The results are reported at 90\%, the highest sparsity target in the PLATON work, and \textit{NA} indicates the model does not converge. The best-performing method is marked in green.}
    \centering
    {\small
    \begin{tabular}{cc|cccccc}
    \toprule 
    Task & \makecell{BERT\\{\scriptsize BASE}} & $l_0$ Regularization & Magnitude & Movement & Soft-Movement & PLATON & \makecell{oBERT\\(ours)} \\
    \midrule
    \makecell{MNLI\\m / mm} & 84.6 / 83.4 & 78.0 / 78.7 & 78.8 / 79.0 & 79.3 / 79.5 & 80.7 / 81.1 & 82.0 / 82.2 & \cellcolor{green!25}\textbf{82.2 / 82.5} \\
    \midrule
    \makecell{QQP\\Acc / F1} & 91.5 / 88.5 & 87.6 / 82.0 & 78.8 / 77.0 & 89.1 / 85.4 & 90.2 / 86.7 & 90.2 / 86.8 & \cellcolor{green!25}\textbf{90.4 / 87.1} \\ 
    \midrule
    \makecell{QNLI\\Acc} & 91.3 & 82.8 & 86.6 & 79.2 & 86.6 & 88.9 & \cellcolor{green!25}\textbf{89.3} \\ 
    \midrule
    \makecell{MRPC\\Acc / F1} & 86.4 / 90.3 & 73.8 / 79.5 & 70.3 / 80.3 & 68.4 / 81.2 & 79.7 / 85.9 & 84.3 / 88.8 & \cellcolor{green!25}\textbf{85.6 / 89.3} \\ 
    \midrule
    \makecell{SST-2\\Acc} & 92.7 & 82.5 & 80.7 & 80.2 & 87.4 & 90.5 & \cellcolor{green!25}\textbf{92.0} \\
    \midrule
    \makecell{CoLA\\Mcc} & 58.3 & NA & NA & NA & NA & 44.3 & \cellcolor{green!25}\textbf{48.47} \\
    \midrule
    \makecell{STS-B\\Pear / Spear} & 90.2 / 89.7 & 82.7 / 83.9 & 83.4 / 83.3 & NA & 86.5 / 86.3 & 87.4 / 87.1 & \cellcolor{green!25}\textbf{88.0 / 87.6} \\
    \bottomrule
    \end{tabular}
    }
\end{table}
\end{document}