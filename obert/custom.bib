@article{Li2020TrainLT,
  title={Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
  author={Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and K. Keutzer and D. Klein and Joseph Gonzalez},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.11794}
}

@inproceedings{yu2022hessian,
  title={Hessian-aware pruning and optimal neural implant},
  author={Yu, Shixing and Yao, Zhewei and Gholami, Amir and Dong, Zhen and Kim, Sehoon and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3880--3891},
  year={2022}
}


@article{Han2016DeepCC,
  title={Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  author={Song Han and Huizi Mao and W. Dally},
  journal={arXiv: Computer Vision and Pattern Recognition},
  year={2016}
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@inproceedings{lagunas21block,
    title = "Block Pruning For Faster Transformers",
    author = "Lagunas, Fran{\c{c}}ois  and
      Charlaix, Ella  and
      Sanh, Victor  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    pages = "10619--10629",
}


@article{Gale2019TheSO,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@article{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@article{hoefler2021,
  author  = {Torsten Hoefler and Dan Alistarh and Tal Ben-Nun and Nikoli Dryden and Alexandra Peste},
  title   = {Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {241},
  pages   = {1-124},
  url     = {http://jmlr.org/papers/v22/21-0366.html}
}

@inproceedings{Devlin2019BERTPO,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
}

@article{Vaswani2017AttentionIA,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{Sang2003IntroductionTT,
  title={Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
  author={E. T. K. Sang and F. D. Meulder},
  booktitle={CoNLL},
  year={2003}
}
@inproceedings{Sun2020MobileBERTAC,
  title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
  author={Zhiqing Sun and Hongkun Yu and Xiaodan Song and Renjie Liu and Yiming Yang and Denny Zhou},
  booktitle={ACL},
  year={2020}
}


@article{Liu2019RoBERTaAR,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{Lewis2020BARTDS,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
  journal={ArXiv},
  year={2020},
  volume={abs/1910.13461}
}
@article{Joshi2020SpanBERTIP,
  title={SpanBERT: Improving Pre-training by Representing and Predicting Spans},
  author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  volume={8},
  pages={64-77}
}
@inproceedings{Xin2020DeeBERTDE,
  title={DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference},
  author={Ji Xin and Raphael Tang and Jaejun Lee and Yaoliang Yu and Jimmy J. Lin},
  booktitle={ACL},
  year={2020}
}
@article{Turc2019WellReadSL,
  title={Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation},
  author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.08962}
}

@article{Blalock2020WhatIT,
  title={What is the State of Neural Network Pruning?},
  author={Davis W. Blalock and J. G. Ortiz and Jonathan Frankle and J. Guttag},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.03033}
}

@article{Prasanna2020WhenBP,
  title={When BERT Plays the Lottery, All Tickets Are Winning},
  author={Sai Prasanna and Anna Rogers and Anna Rumshisky},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00561}
}
@article{Zhu2018ToPO,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={M. Zhu and Suyog Gupta},
  journal={ArXiv},
  year={2018},
  volume={abs/1710.01878}
}

@inproceedings{Shankar2017IdentifyingQQ,
  title={Identifying Quora question pairs having the same intent},
  author={S. Shankar},
  year={2017}
}
@article{McCarley2019StructuredPO,
  title={Structured Pruning of a BERT-based Question Answering Model},
  author={J. Scott McCarley and Rishav Chakravarti and Avirup Sil},
  journal={arXiv: Computation and Language},
  year={2019}
}
@inproceedings{Bucila2006ModelC,
  title={Model compression},
  author={Cristian Bucila and R. Caruana and Alexandru Niculescu-Mizil},
  booktitle={KDD '06},
  year={2006}
}
@article{Wang2018GLUEAM,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.07461}
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
}
@inproceedings{Wang2020StructuredPO,
  title={Structured Pruning of Large Language Models},
  author={Ziheng Wang and Jeremy Wohlwend and Tao Lei},
  booktitle={EMNLP},
  year={2020}
}
@article{Gong2014CompressingDC,
  title={Compressing Deep Convolutional Networks using Vector Quantization},
  author={Yunchao Gong and L. Liu and Ming Yang and Lubomir D. Bourdev},
  journal={ArXiv},
  year={2014},
  volume={abs/1412.6115}
}

@article{Sanh2020MovementPA,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@inproceedings{Xu2021RethinkingNP,
  title={Rethinking Network Pruning â€“ under the Pre-train and Fine-tune Paradigm},
  author={Dongkuan Xu and Ian En-Hsu Yen and Jinxi Zhao and Zhibin Xiao},
  booktitle={NAACL},
  year={2021}
}

@article{LeCun1989OptimalBD,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{DBLP:journals/corr/abs-2010-10499,
  author    = {Adrian de Wynter and
               Daniel J. Perry},
  title     = {Optimal Subarchitecture Extraction For {BERT}},
  journal   = {CoRR},
  volume    = {abs/2010.10499},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.10499},
  eprinttype = {arXiv},
  eprint    = {2010.10499},
  timestamp = {Mon, 26 Oct 2020 15:39:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-10499.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2105-06990,
  author    = {Olga Kovaleva and
               Saurabh Kulshreshtha and
               Anna Rogers and
               Anna Rumshisky},
  title     = {{BERT} Busters: Outlier LayerNorm Dimensions that Disrupt {BERT}},
  journal   = {CoRR},
  volume    = {abs/2105.06990},
  year      = {2021},
  eprinttype = {arXiv},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-06990.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2004-03844,
  author    = {Hassan Sajjad and
               Fahim Dalvi and
               Nadir Durrani and
               Preslav Nakov},
  title     = {Poor Man's {BERT:} Smaller and Faster Transformer Models},
  journal   = {CoRR},
  volume    = {abs/2004.03844},
  year      = {2020},
}

@inproceedings{Frankle2019TheLT,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{Sridhar2020UndividedAA,
  title={Undivided Attention: Are Intermediate Layers Necessary for BERT?},
  author={Sridhar, Sharath Nittur and Sarah, Anthony},
  journal={arXiv preprint arXiv:2012.11881},
  year={2020}
}

@article{Courbariaux2016BinarizedNN,
  title={Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1},
  author={Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1602.02830},
  year={2016}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "38--45"
}

@article{Radford2019LanguageMA,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{Jiao2020TinyBERTDB,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={4163--4174},
  year={2020}
}

@misc{Chen2020TheLT,
    title={The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
    author={Tianlong Chen and Jonathan Frankle and Shiyu Chang and Sijia Liu and Yang Zhang and Zhangyang Wang and Michael Carbin},
    year={2020},
    eprint={2007.12223},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{chen2020lottery,
  title={The lottery ticket hypothesis for pre-trained bert networks},
  author={Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={15834--15846},
  year={2020}
}

@article{Singh2020WoodFisherES,
  title={WoodFisher: Efficient Second-Order Approximation for Neural Network Compression},
  author={Singh, Sidak Pal and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{Frantar2021EfficientMA,
  title={M-FAC: Efficient Matrix-Free Approximations of Second-Order Information},
  author={Frantar, Elias and Kurtic, Eldar and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{Merity2017PointerSM,
  title={Pointer Sentinel Mixture Models},
  author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
  journal={ArXiv},
  year={2017},
  volume={abs/1609.07843}
}
@article{Zhu2015AligningBA,
  title={Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  author={Yukun Zhu and Ryan Kiros and Richard S. Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
  journal={2015 IEEE International Conference on Computer Vision (ICCV)},
  year={2015},
  pages={19-27}
}
@inproceedings{Voita2019AnalyzingMS,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Elena Voita and David Talbot and F. Moiseev and Rico Sennrich and Ivan Titov},
  booktitle={ACL},
  year={2019}
}
@article{shankar2017first,
  title={First Quora Dataset Release: Question Pairs},
  author={Shankar, Iyer and Nikhil, Dandekar and Kornel, Csernai},
  year={2017}
}
@article{MTNLG,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{Wang2020MiniLMDS,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5776--5788},
  year={2020}
}

@article{Michel2019AreSH,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{Zafrir2019Q8BERTQ8,
  title={Q8BERT: Quantized 8Bit BERT},
  author={Ofir Zafrir and Guy Boudoukh and Peter Izsak and Moshe Wasserblat},
  journal={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)},
  year={2019},
  pages={36-39}
}
@inproceedings{Rajpurkar2016SQuAD1Q,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle={EMNLP},
  year={2016}
}

@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{Li2020TrainLT,
  title={Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
  author={Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and K. Keutzer and D. Klein and Joseph Gonzalez},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.11794}
}

@article{Han2016DeepCC,
  title={Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  author={Song Han and Huizi Mao and W. Dally},
  journal={arXiv: Computer Vision and Pattern Recognition},
  year={2016}
}

@article{zhang2020ternarybert,
  title={Ternarybert: Distillation-aware ultra-low bit bert},
  author={Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2009.12812},
  year={2020}
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={8815--8821},
  year={2020}
}

@article{fan2020training,
  title={Training with quantization noise for extreme model compression},
  author={Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, R{\'e}mi and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:2004.07320},
  year={2020}
}


@article{Hinton2015DistillingTK,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@inproceedings{fan2019reducing,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{Sang2003IntroductionTT,
  title={Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
  author={E. T. K. Sang and F. D. Meulder},
  booktitle={CoNLL},
  year={2003}
}

@article{Blalock2020WhatIT,
  title={What is the State of Neural Network Pruning?},
  author={Davis W. Blalock and J. G. Ortiz and Jonathan Frankle and J. Guttag},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.03033}
}


@inproceedings{liu2021group,
  title={Group fisher pruning for practical network compression},
  author={Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  booktitle={International Conference on Machine Learning},
  pages={7021--7032},
  year={2021},
  organization={PMLR}
}


@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}


@article{McCarley2019StructuredPO,
  title={Structured Pruning of a BERT-based Question Answering Model},
  author={J. Scott McCarley and Rishav Chakravarti and Avirup Sil},
  journal={arXiv: Computation and Language},
  year={2019}
}
@inproceedings{Bucila2006ModelC,
  title={Model compression},
  author={Cristian Bucila and R. Caruana and Alexandru Niculescu-Mizil},
  booktitle={KDD '06},
  year={2006}
}

@ONLINE {wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}
@article{Wang2018GLUEAM,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.07461}
}

@inproceedings{Wang2020StructuredPO,
  title={Structured Pruning of Large Language Models},
  author={Ziheng Wang and Jeremy Wohlwend and Tao Lei},
  booktitle={EMNLP},
  year={2020}
}
@article{Gong2014CompressingDC,
  title={Compressing Deep Convolutional Networks using Vector Quantization},
  author={Yunchao Gong and L. Liu and Ming Yang and Lubomir D. Bourdev},
  journal={ArXiv},
  year={2014},
  volume={abs/1412.6115}
}
@article{Han2015ADN,
  title={A deep neural network compression pipeline: Pruning, quantization, huffman encoding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  volume={10},
  year={2015}
}

@misc{deepsparse,
      title={Deep Sparse: A Fast CPU Inference Engine}, 
      author={NeuralMagic},
      year={2021},
      eprint={https://github.com/neuralmagic/deepsparse},
}

@inproceedings{Shen2020QBERTHB,
  title={Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT},
  author={Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
  booktitle={AAAI},
  year={2020}
}

@article{zafrir2021prune,
  title={Prune once for all: Sparse pre-trained language models},
  author={Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen, Haihao and Wasserblat, Moshe},
  journal={arXiv preprint arXiv:2111.05754},
  year={2021}
}

@InProceedings{
    pmlr-v119-kurtz20a, 
    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks}, 
    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan}, 
    booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
    pages = {5533--5543}, 
    year = {2020}, 
    editor = {Hal DaumÃ© III and Aarti Singh}, 
    volume = {119}, 
    series = {Proceedings of Machine Learning Research}, 
    address = {Virtual}, 
    month = {13--18 Jul}, 
    publisher = {PMLR}, 
}

@inproceedings{hf-datasets,
    title = "Datasets: A Community Library for Natural Language Processing",
    author = "Lhoest, Quentin  and
      Villanova del Moral, Albert  and
      Jernite, Yacine  and
      Thakur, Abhishek  and
      von Platen, Patrick  and
      Patil, Suraj  and
      Chaumond, Julien  and
      Drame, Mariama  and
      Plu, Julien  and
      Tunstall, Lewis  and
      Davison, Joe  and
      {\v{S}}a{\v{s}}ko, Mario  and
      Chhablani, Gunjan  and
      Malik, Bhavitvya  and
      Brandeis, Simon  and
      Le Scao, Teven  and
      Sanh, Victor  and
      Xu, Canwen  and
      Patry, Nicolas  and
      McMillan-Major, Angelina  and
      Schmid, Philipp  and
      Gugger, Sylvain  and
      Delangue, Cl{\'e}ment  and
      Matussi{\`e}re, Th{\'e}o  and
      Debut, Lysandre  and
      Bekman, Stas  and
      Cistac, Pierric  and
      Goehringer, Thibault  and
      Mustar, Victor  and
      Lagunas, Fran{\c{c}}ois  and
      Rush, Alexander  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    pages = "175--184",
    archivePrefix={arXiv},
    primaryClass={cs.CL},
}

@inproceedings{kim2021learned,
author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
title = {Learned Token Pruning for Transformers},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {784â€“794},
series = {KDD '22}
}

@inproceedings{liu2020fastbert,
  title={FastBERT: a Self-distilling BERT with Adaptive Inference Time},
  author={Weijie, Liu and Peng, Zhou and Zhe, Zhao and Zhiruo, Wang and Haotang, Deng and Qi, Ju},
  booktitle={Proceedings of ACL 2020},
  year={2020}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{mishra2021accelerating,
  title={Accelerating sparse deep neural networks},
  author={Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2104.08378},
  year={2021}
}

@inproceedings{zhang2022platon,
  title={Platon: Pruning large transformer models with upper confidence bound of weight importance},
  author={Zhang, Qingru and Zuo, Simiao and Liang, Chen and Bukharin, Alexander and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={26809--26823},
  year={2022},
  organization={PMLR}
}

@inproceedings{sun2020mobilebert,
  title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={2158--2170},
  year={2020}
}