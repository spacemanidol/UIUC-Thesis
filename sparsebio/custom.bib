@article{Li2020TrainLT,
  title={Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
  author={Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and K. Keutzer and D. Klein and Joseph Gonzalez},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.11794}
}

@article{Han2016DeepCC,
  title={Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  author={Song Han and Huizi Mao and W. Dally},
  journal={arXiv: Computer Vision and Pattern Recognition},
  year={2016}
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}
@inproceedings{Gordon2020CompressingBS,
  title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning},
  author={Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},
  booktitle={Workshop on Representation Learning for NLP},
  year={2020}
}
@inproceedings{lagunas21block,
    title = "Block Pruning For Faster Transformers",
    author = "Lagunas, Fran{\c{c}}ois  and
      Charlaix, Ella  and
      Sanh, Victor  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.829",
    doi = "10.18653/v1/2021.emnlp-main.829",
    pages = "10619--10629",
}


@article{Gale2019TheSO,
  title={The State of Sparsity in Deep Neural Networks},
  author={Trevor Gale and Erich Elsen and Sara Hooker},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.09574}
}

@book{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  year={1993},
  publisher={Morgan Kaufmann}
}

@article{hoefler2021,
  author  = {Torsten Hoefler and Dan Alistarh and Tal Ben-Nun and Nikoli Dryden and Alexandra Peste},
  title   = {Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {241},
  pages   = {1-124},
  url     = {http://jmlr.org/papers/v22/21-0366.html}
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={J. Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL},
  year={2019}
}
@article{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.03762}
}
@inproceedings{Sang2003IntroductionTT,
  title={Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
  author={E. T. K. Sang and F. D. Meulder},
  booktitle={CoNLL},
  year={2003}
}
@inproceedings{Sun2020MobileBERTAC,
  title={MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},
  author={Zhiqing Sun and Hongkun Yu and Xiaodan Song and Renjie Liu and Yiming Yang and Denny Zhou},
  booktitle={ACL},
  year={2020}
}

@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}
@article{Lewis2020BARTDS,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Veselin Stoyanov and Luke Zettlemoyer},
  journal={ArXiv},
  year={2020},
  volume={abs/1910.13461}
}
@article{Joshi2020SpanBERTIP,
  title={SpanBERT: Improving Pre-training by Representing and Predicting Spans},
  author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  volume={8},
  pages={64-77}
}
@inproceedings{Xin2020DeeBERTDE,
  title={DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference},
  author={Ji Xin and Raphael Tang and Jaejun Lee and Yaoliang Yu and Jimmy J. Lin},
  booktitle={ACL},
  year={2020}
}
@article{Turc2019WellReadSL,
  title={Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation},
  author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.08962}
}

@article{Blalock2020WhatIT,
  title={What is the State of Neural Network Pruning?},
  author={Davis W. Blalock and J. G. Ortiz and Jonathan Frankle and J. Guttag},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.03033}
}

@article{Prasanna2020WhenBP,
  title={When BERT Plays the Lottery, All Tickets Are Winning},
  author={Sai Prasanna and Anna Rogers and Anna Rumshisky},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00561}
}
@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165}
}
@inproceedings{Williams2018ABC,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Adina Williams and Nikita Nangia and Samuel R. Bowman},
  booktitle={NAACL},
  year={2018}
}
@article{Zhu2018ToPO,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={M. Zhu and Suyog Gupta},
  journal={ArXiv},
  year={2018},
  volume={abs/1710.01878}
}
@inproceedings{Shankar2017IdentifyingQQ,
  title={Identifying Quora question pairs having the same intent},
  author={S. Shankar},
  year={2017}
}
@article{McCarley2019StructuredPO,
  title={Structured Pruning of a BERT-based Question Answering Model},
  author={J. Scott McCarley and Rishav Chakravarti and Avirup Sil},
  journal={arXiv: Computation and Language},
  year={2019}
}
@inproceedings{Bucila2006ModelC,
  title={Model compression},
  author={Cristian Bucila and R. Caruana and Alexandru Niculescu-Mizil},
  booktitle={KDD '06},
  year={2006}
}
@article{Wang2018GLUEAM,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.07461}
}
@article{Sajjad2020PoorMB,
  title={Poor Man's BERT: Smaller and Faster Transformer Models},
  author={Hassan Sajjad and Fahim Dalvi and Nadir Durrani and Preslav Nakov},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.03844}
}
@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}
@inproceedings{Wang2020StructuredPO,
  title={Structured Pruning of Large Language Models},
  author={Ziheng Wang and Jeremy Wohlwend and Tao Lei},
  booktitle={EMNLP},
  year={2020}
}
@article{Gong2014CompressingDC,
  title={Compressing Deep Convolutional Networks using Vector Quantization},
  author={Yunchao Gong and L. Liu and Ming Yang and Lubomir D. Bourdev},
  journal={ArXiv},
  year={2014},
  volume={abs/1412.6115}
}
@article{Sanh2020MovementPA,
  title={Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  author={Victor Sanh and Thomas Wolf and Alexander M. Rush},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.07683}
}

@inproceedings{Xu2021RethinkingNP,
  title={Rethinking Network Pruning â€“ under the Pre-train and Fine-tune Paradigm},
  author={Dongkuan Xu and Ian En-Hsu Yen and Jinxi Zhao and Zhibin Xiao},
  booktitle={NAACL},
  year={2021}
}
@inproceedings{LeCun1989OptimalBD,
  title={Optimal Brain Damage},
  author={Yann LeCun and John S. Denker and Sara A. Solla},
  booktitle={NIPS},
  year={1989}
}
@article{DBLP:journals/corr/abs-2010-10499,
  author    = {Adrian de Wynter and
               Daniel J. Perry},
  title     = {Optimal Subarchitecture Extraction For {BERT}},
  journal   = {CoRR},
  volume    = {abs/2010.10499},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.10499},
  eprinttype = {arXiv},
  eprint    = {2010.10499},
  timestamp = {Mon, 26 Oct 2020 15:39:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-10499.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2105-06990,
  author    = {Olga Kovaleva and
               Saurabh Kulshreshtha and
               Anna Rogers and
               Anna Rumshisky},
  title     = {{BERT} Busters: Outlier LayerNorm Dimensions that Disrupt {BERT}},
  journal   = {CoRR},
  volume    = {abs/2105.06990},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.06990},
  eprinttype = {arXiv},
  eprint    = {2105.06990},
  timestamp = {Tue, 18 May 2021 18:46:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-06990.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2004-03844,
  author    = {Hassan Sajjad and
               Fahim Dalvi and
               Nadir Durrani and
               Preslav Nakov},
  title     = {Poor Man's {BERT:} Smaller and Faster Transformer Models},
  journal   = {CoRR},
  volume    = {abs/2004.03844},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.03844},
  eprinttype = {arXiv},
  eprint    = {2004.03844},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-03844.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Frankle2019TheLT,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Jonathan Frankle and Michael Carbin},
  journal={arXiv: Learning},
  year={2019}
}
@article{Sridhar2020UndividedAA,
  title={Undivided Attention: Are Intermediate Layers Necessary for BERT?},
  author={Sharath Nittur Sridhar and Anthony Sarah},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.11881}
}

@article{Courbariaux2016BinarizedNN,
  title={Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1},
  author={Matthieu Courbariaux and Itay Hubara and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
  journal={arXiv: Learning},
  year={2016}
}
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}
@article{Jiao2020TinyBERTDB,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
  journal={ArXiv},
  year={2020},
  volume={abs/1909.10351}
}
@misc{Chen2020TheLT,
    title={The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
    author={Tianlong Chen and Jonathan Frankle and Shiyu Chang and Sijia Liu and Yang Zhang and Zhangyang Wang and Michael Carbin},
    year={2020},
    eprint={2007.12223},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Singh2020WoodFisherES,
  title={WoodFisher: Efficient Second-Order Approximation for Neural Network Compression},
  author={Singh, Sidak Pal and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{Frantar2021EfficientMA,
  title={M-FAC: Efficient Matrix-Free Approximations of Second-Order Information},
  author={Frantar, Elias and Kurtic, Eldar and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{Merity2017PointerSM,
  title={Pointer Sentinel Mixture Models},
  author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
  journal={ArXiv},
  year={2017},
  volume={abs/1609.07843}
}
@article{Zhu2015AligningBA,
  title={Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  author={Yukun Zhu and Ryan Kiros and Richard S. Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
  journal={2015 IEEE International Conference on Computer Vision (ICCV)},
  year={2015},
  pages={19-27}
}
@inproceedings{Voita2019AnalyzingMS,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Elena Voita and David Talbot and F. Moiseev and Rico Sennrich and Ivan Titov},
  booktitle={ACL},
  year={2019}
}
@inproceedings{Guo2017DuplicateQQ,
  title={Duplicate Quora Questions Detection},
  author={Lei Guo and H. Tian},
  year={2017}
}
@misc{MTNLG,
  title = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the Worldâ€™s Largest and Most Powerful Generative Language Model},
  howpublished = {\url{https://bit.ly/3DlbPIF/}},
  note = {Accessed: 2021-11-09}
}
@article{Wang2020MiniLMDS,
  title={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
  author={Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.10957}
}
@inproceedings{Michel2019AreSH,
  title={Are Sixteen Heads Really Better than One?},
  author={Paul Michel and Omer Levy and Graham Neubig},
  booktitle={NeurIPS},
  year={2019}
}
@article{Zafrir2019Q8BERTQ8,
  title={Q8BERT: Quantized 8Bit BERT},
  author={Ofir Zafrir and Guy Boudoukh and Peter Izsak and Moshe Wasserblat},
  journal={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)},
  year={2019},
  pages={36-39}
}
@inproceedings{Rajpurkar2016SQuAD1Q,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle={EMNLP},
  year={2016}
}
@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}
@article{Li2020TrainLT,
  title={Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
  author={Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and K. Keutzer and D. Klein and Joseph Gonzalez},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.11794}
}

@article{Han2016DeepCC,
  title={Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  author={Song Han and Huizi Mao and W. Dally},
  journal={arXiv: Computer Vision and Pattern Recognition},
  year={2016}
}

@article{zhang2020ternarybert,
  title={Ternarybert: Distillation-aware ultra-low bit bert},
  author={Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2009.12812},
  year={2020}
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8815--8821},
  year={2020}
}

@article{fan2020training,
  title={Training with quantization noise for extreme model compression},
  author={Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, R{\'e}mi and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:2004.07320},
  year={2020}
}

@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and J. Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@article{fan2019reducing,
  title={Reducing transformer depth on demand with structured dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1909.11556},
  year={2019}
}

@inproceedings{Sang2003IntroductionTT,
  title={Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
  author={E. T. K. Sang and F. D. Meulder},
  booktitle={CoNLL},
  year={2003}
}
@inproceedings{Xin2020DeeBERTDE,
  title={DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference},
  author={Ji Xin and Raphael Tang and Jaejun Lee and Yaoliang Yu and Jimmy J. Lin},
  booktitle={ACL},
  year={2020}
}

@article{Blalock2020WhatIT,
  title={What is the State of Neural Network Pruning?},
  author={Davis W. Blalock and J. G. Ortiz and Jonathan Frankle and J. Guttag},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.03033}
}

@inproceedings{Williams2018ABC,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Adina Williams and Nikita Nangia and Samuel R. Bowman},
  booktitle={NAACL},
  year={2018}
}

@inproceedings{liu2021group,
  title={Group fisher pruning for practical network compression},
  author={Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  booktitle={International Conference on Machine Learning},
  pages={7021--7032},
  year={2021},
  organization={PMLR}
}


@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}


@article{McCarley2019StructuredPO,
  title={Structured Pruning of a BERT-based Question Answering Model},
  author={J. Scott McCarley and Rishav Chakravarti and Avirup Sil},
  journal={arXiv: Computation and Language},
  year={2019}
}
@inproceedings{Bucila2006ModelC,
  title={Model compression},
  author={Cristian Bucila and R. Caruana and Alexandru Niculescu-Mizil},
  booktitle={KDD '06},
  year={2006}
}
@misc {wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org",
    year={2021}
}
@article{Wang2018GLUEAM,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.07461}
}
@article{Sajjad2020PoorMB,
  title={Poor Man's BERT: Smaller and Faster Transformer Models},
  author={Hassan Sajjad and Fahim Dalvi and Nadir Durrani and Preslav Nakov},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.03844}
}
@inproceedings{Wang2020StructuredPO,
  title={Structured Pruning of Large Language Models},
  author={Ziheng Wang and Jeremy Wohlwend and Tao Lei},
  booktitle={EMNLP},
  year={2020}
}
@article{Gong2014CompressingDC,
  title={Compressing Deep Convolutional Networks using Vector Quantization},
  author={Yunchao Gong and L. Liu and Ming Yang and Lubomir D. Bourdev},
  journal={ArXiv},
  year={2014},
  volume={abs/1412.6115}
}
 @inproceedings{Kovaleva2021BERTBO,
  title={BERT Busters: Outlier Dimensions that Disrupt Transformers},
  author={Olga Kovaleva and Saurabh Kulshreshtha and Anna Rogers and Anna Rumshisky},
  booktitle={FINDINGS},
  year={2021}
}
@inproceedings{Jin2019PubMedQAAD,
  title={PubMedQA: A Dataset for Biomedical Research Question Answering},
  author={Qiao Jin and Bhuwan Dhingra and Zhengping Liu and William W. Cohen and Xinghua Lu},
  booktitle={EMNLP},
  year={2019}
}
@article{Baker2016AutomaticSC,
  title={Automatic semantic classification of scientific literature according to the hallmarks of cancer},
  author={Simon Baker and Ilona Silins and Yufan Guo and Imran Ali and Johan H{\"o}gberg and Ulla Stenius and Anna Korhonen},
  journal={Bioinformatics},
  year={2016},
  volume={32 3},
  pages={
          432-40
        }
}
@article{Becker2004TheGA,
  title={The Genetic Association Database},
  author={Kevin G. Becker and Kathleen C. Barnes and Tiffani J. Bright and S Alex Wang},
  journal={Nature Genetics},
  year={2004},
  volume={36},
  pages={431-432}
}
@article{HerreroZazo2013TheDC,
  title={The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions},
  author={Mar{\'i}a Herrero-Zazo and Isabel Segura-Bedmar and Paloma Mart{\'i}nez and Thierry Declerck},
  journal={Journal of biomedical informatics},
  year={2013},
  volume={46 5},
  pages={
          914-20
        }
}
@article{Taboureau2011ChemProtAD,
  title={ChemProt: a disease chemical biology database},
  author={Olivier Taboureau and Sonny Kim Nielsen and Karine Audouze and Nils Weinhold and Daniel Edsg{\"a}rd and Francisco S. Roque and Irene Kouskoumvekaki and Alina Bora and Ramona Curpan and Thomas Sk{\o}t Jensen and S{\o}ren Brunak and Tudor I. Oprea},
  journal={Nucleic Acids Research},
  year={2011},
  volume={39},
  pages={D367 - D372}
}
@article{Li2016BioCreativeVC,
  title={BioCreative V CDR task corpus: a resource for chemical disease relation extraction},
  author={Jiao Li and Yueping Sun and Robin J. Johnson and Daniela Sciaky and Chih-Hsuan Wei and Robert Leaman and Allan Peter Davis and Carolyn J. Mattingly and Thomas C. Wiegers and Zhiyong Lu},
  journal={Database: The Journal of Biological Databases and Curation},
  year={2016},
  volume={2016}
}
@article{Dogan2014NCBIDC,
  title={NCBI disease corpus: A resource for disease name recognition and concept normalization},
  author={Rezarta Islamaj Dogan and Robert Leaman and Zhiyong Lu},
  journal={Journal of biomedical informatics},
  year={2014},
  volume={47},
  pages={
          1-10
        }
}
@inproceedings{Collier2004IntroductionTT,
  title={Introduction to the Bio-entity Recognition Task at JNLPBA},
  author={Nigel Collier and Jin-Dong Kim},
  booktitle={NLPBA/BioNLP},
  year={2004}
}
@article{Kurti2022TheOB,
  title={The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models},
  author={Eldar Kurti{\'c} and Daniel Fernando Campos and Tuan Nguyen and Elias Frantar and Mark Kurtz and Ben Fineran and Michael Goin and Dan Alistarh},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.07259}
}
@InProceedings{Zhu_2015_ICCV,
    title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month = {December},
    year = {2015}
}

@article{Smith2008OverviewOB,
  title={Overview of BioCreative II gene mention recognition},
  author={Larry L. Smith and Lorraine K. Tanabe and Rie Johnson nee Ando and Cheng-Ju Kuo and I-Fang Chung and Chun-Nan Hsu and Yu-Shi Lin and Roman Klinger and C. Friedrich and Kuzman Ganchev and Manabu Torii and Hongfang Liu and Barry Haddow and Craig A. Struble and Richard J. Povinelli and Andreas Vlachos and William A. Baumgartner and Lawrence E. Hunter and Bob Carpenter and Richard Tzong-Han Tsai and Hong-Jie Dai and Feng Liu and Yifei Chen and Chengjie Sun and Sophia Katrenko and Pieter W. Adriaans and Christian Blaschke and Rafael Torres and Mariana L. Neves and Preslav Nakov and Anna Divoli and Manuel Ma{\~n}a-L{\'o}pez and Jacinto Mata and W. John Wilbur},
  journal={Genome Biology},
  year={2008},
  volume={9},
  pages={S2 - S2}
}
@article{Gu2022DomainSpecificLM,
  title={Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
  author={Yuxian Gu and Robert Tinn and Hao Cheng and Michael R. Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  year={2022},
  volume={3},
  pages={1 - 23}
}
@inproceedings{Kanakarajan2021BioELECTRAPretrainedBT,
  title={BioELECTRA:Pretrained Biomedical text Encoder using Discriminators},
  author={Kamal Raj Kanakarajan and Bhuvana Kundumani and Malaikannan Sankarasubbu},
  booktitle={BIONLP},
  year={2021}
}
 @article{Chen2021CrossViTCM,
  title={CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification},
  author={Chun-Fu Chen and Quanfu Fan and Rameswar Panda},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={347-356}
}
 @article{Arnab2021ViViTAV,
  title={ViViT: A Video Vision Transformer},
  author={Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lucic and Cordelia Schmid},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={6816-6826}
}
@article{Han2015ADN,
  title={A Deep Neural Network Compression Pipeline: Pruning, Quantization, Huffman Encoding},
  author={Song Han and Huizi Mao and William J. Dally},
  year={2015},
  journal={ArXiv}
}
@article{Chalkidis2020LEGALBERTTM,
  title={LEGAL-BERT: The Muppets straight out of Law School},
  author={Ilias Chalkidis and Manos Fergadiotis and Prodromos Malakasiotis and Nikolaos Aletras and Ion Androutsopoulos},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.02559}
}
@article{Lee2020BioBERTAP,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},
  journal={Bioinformatics},
  year={2020},
  volume={36},
  pages={1234 - 1240}
}
@inproceedings{Beltagy2019SciBERTAP,
  title={SciBERT: A Pretrained Language Model for Scientific Text},
  author={Iz Beltagy and Kyle Lo and Arman Cohan},
  booktitle={EMNLP},
  year={2019}
}

@article{Peng2021RandomFA,
  title={Random Feature Attention},
  author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah A. Smith and Lingpeng Kong},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.02143}
}
@article{Bommasani2021OnTO,
  title={On the Opportunities and Risks of Foundation Models},
  author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07258}
}
@article{Zafrir2021PruneOF,
  title={Prune Once for All: Sparse Pre-Trained Language Models},
  author={Ofir Zafrir and Ariel Larey and Guy Boudoukh and Haihao Shen and Moshe Wasserblat},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.05754}
}
@misc{deepsparse,
      title={Deep Sparse: A Fast CPU Inference Engine}, 
      author={NeuralMagic},
      year={2021},
      eprint={https://github.com/neuralmagic/deepsparse},
}

@inproceedings{Shen2020QBERTHB,
  title={Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT},
  author={Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
  booktitle={AAAI},
  year={2020}
}
@misc{zafrir2021prune,
      title={Prune Once for All: Sparse Pre-Trained Language Models}, 
      author={Ofir Zafrir and Ariel Larey and Guy Boudoukh and Haihao Shen and Moshe Wasserblat},
      year={2021},
      eprint={2111.05754},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}