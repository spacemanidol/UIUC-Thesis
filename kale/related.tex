\textbf{Transformer Based Language Models} such as BERT \cite{Devlin2019BERTPO} provide contextual language representations built on the Transformer architecture \cite{Vaswani2017AttentionIA} which can be specialized and adapted for specific tasks and domains \cite{Lee2020BioBERTAP}. Using contextual word representations, it becomes relatively easy to excel at a broad range of natural language processing tasks such as Question Answering, Text Classification, and sentiment analysis. \\
\textbf{Bi-Encoders}, commonly called dual-encoders or dense retrievers, decompose ranking by leveraging the inner product of query and document representations to produce a relevance score for query document pairs. While not as accurate at cross-encoders \cite{Reimers2019SentenceBERTSE}, they are more efficient for inference and easier to deploy. Bi-encoder document representations are query invariant, allowing them to be pre-computed and loaded into an Approximate Nearest Neighbor (ANN) such as FAISS \cite{johnson2019billion}. \\
At runtime, a query is an encoder into a latent space, and the $k$ documents are retrieved using a nearest neighbor algorithm such as HNSW \cite{Malkov2016EfficientAR}. Since the entire document index has been pre-computed, the retrieval latency is limited to a single call of the document encoder. \\
Bi-encoders commonly leverage LLM such as BERT \cite{Devlin2019BERTPO} to retrieve short passages of text leading to the task descriptor of Dense Passage Retrievers (DPR) \cite{Karpukhin2020DensePR}. Driven by their efficiency in deployment and relevance performance, DPR-based models have rapidly become the building blocks for systems doing product search \cite{Magnani2022SemanticRA}, open domain question answering \cite{Karpukhin2020DensePR} and customer support \cite{Mesquita2022DenseTR}.\\
\textbf{Efficient Inference} study methods and models which decrease the model execution cost while minimizing the losses to model performance. \\
Knowledge Distillation \cite{Hinton2015DistillingTK} is a training method where a model, called the \textit{student}, learns to emulate a \textit{teacher} model, which is commonly larger or better performing than the \textit{student}.\\
Unstructured pruning removes individual weights or groups of weights in a model by applying a mask or setting the weight values to 0. When paired with a sparsity-aware inference engine, it is possible to gain 3-5x speedups in inference throughput with little to no loss in accuracy \cite{Kurti2022TheOB}. \\
Structured pruning removes fundamental structural components in a language model, such as individual attention heads \cite{Voita2019AnalyzingMS} or entire model layers \cite{sanh2019distilbert}. Removing entire model layers is one of the most pervasive approaches, as latency gains are easy to realize, and pruning is straightforward. \\
While their training regimes may differ, models like DistilBERT \cite{sanh2019distilbert} and TinyBERT \cite{Jiao2020TinyBERTDB}, and MiniLM \cite{wang2020minilm} leverage structural pruning as ways of generation 2-10x speedups. \\
Methods like quantization \cite{Pouransari2020LeastSB} \cite{Zafrir2019Q8BERTQ8}, early exiting \cite{Xin2020DeeBERTDE} or token pruning \cite{Kim2021LearnedTP} have been effective in other NLP tasks. Still, our work primarily focuses on structured pruning and its relationship with asymmetry. We leave studying the impacts of asymmetry on these compression methods to future work.  \\
\textbf{Asymmetrical deep learning} broadly refers to any non-uniformity in shape or attribute of models. Traditional modeling approaches favor uniformity as it is preferable for optimization algorithms \cite{Mihaylova2019ScheduledSF}, and using models for inference should match training as closely as possible \cite{Ranzato2015SequenceLT} as improvements in training loss during optimization result in improvements in model performance during inference. However, this does not account for cost or latency asymmetries during usage. Kasai et al. demonstrated how the sequence-to-sequence encoder depth dominates language model performance for machine translation \cite{Kasai2020DeepES}. Tay et al. 2021 extend this work by finding a \textit{DeepNarrow} which shows that for broad language modeling, it is possible to have 50\% fewer parameters and a 40\% faster inference with no loss in accuracy.\\
\textbf{Embedding Distillation}
Concurrent to our work on bi-encoder compression, Kim et al. 2023 study how distillation in embeddings leads to general compression of bi-encoders and cross-encoders \cite{Kim2023EmbedDistillAG}. Our work differs from theirs as we focus on the role of asymmetry between query and document encoders and how to leverage it for improved inference efficiency.


