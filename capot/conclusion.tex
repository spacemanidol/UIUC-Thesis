This work studies how to improve bi-encoder performance on noisy queries efficiently. By extending the contrastive triplet loss with an anchoring loss, CAPOT can be used as an approximation for data augmentation without the associated computational overhead. By avoiding retraining and alternating the training corpus, CAPOT can significantly improve recall accuracy with 20 times less computational overhead.  \\ 
In future work, we wish to study how representation alignment approaches can be used with compression approaches such as distillation, pruning, and quantization.