\textbf{Bi-Encoders}, commonly called dual-encoders or dense retrievers, decompose ranking by leveraging the inner product of query and document representations to produce a relevance score for query document pairs. Since their document representations are query invariant, they can be pre-computed and loaded into an Approximate Nearest Neighbor (ANN) such as FAISS \cite{johnson2019billion}. The $k$ closest documents can be found for each query with minimal latency at run time. Since bi-encoders leverage LLM such as BERT \cite{Devlin2019BERTPO}, they are often limited to ranking short passages of text and are commonly referred to as Dense Passage Retrievers (DPR) \cite{Karpukhin2020DensePR}. Driven by their efficiency in deployment and relevance performance, DPR-based models have rapidly become the building blocks for systems doing product search \cite{Magnani2022SemanticRA}, open domain question answering \cite{Karpukhin2020DensePR} and customer support \cite{Mesquita2022DenseTR}.\\
Recent work has heavily focused on improving the relevance of DPR models by improving the negative sampling using methods like ANCE \cite{Xiong2021ApproximateNN} and in-batch negatives \cite{Lin2021InBatchNF}. While effective DPR models are brittle to shifts in the domain, minor variations can cause a complete collapse in relevance. Li et al. '2022 introduced methods for improving such performance by having a single query encoder leverage multiple document encoders to transfer between domains \cite{Li2022AnEA}. While effective, such a method carries a high computational load as multiple indexes must be maintained and updated. \\
\textbf{Data Augmentation} (DA) is a popular approach for improving how well models perform on new or noisy data. In data augmentation, training is extended by augmenting the training data with modifications or perturbations which match the desired model behavior. DA is extremely common in computer vision where training data is commonly rotated, blurred, cropped, or zoomed-in/out \cite{Mikoajczyk2018DataAF} \cite{Zhong2020RandomED}. \\
DA has become increasingly more popular in NLP and has been used to improve model performance \cite{Jiao2020TinyBERTDB}, simulate large-scale training data when it is not available \cite{Li2020ADD}, and mitigate bias \cite{Lu2020GenderBI} in existing datasets. A detailed survey on DA approaches for NLP has been complied by Feng et al. 21' \cite{Feng2021ASO}.\\
\textbf{Contrastive Learning} builds on the notion of a contrastive loss \cite{Chopra2005LearningAS}, which seeks to create clusters in the embedding space such that examples with a shared class are far from other classes but close to each other. Much like learning that queries with noise have a shared intent, Schroff et al. 15' leverage contrastive learning to recognize faces despite different angles and perspectives \cite{Schroff2015FaceNetAU} by using a triplet loss. This approach is a natural fit for the world of search as relevance is at its core clustering relevant items close together and far from irrelevant items. Recently, contrastive learning has become a method for learning relevance at the corpora scale \cite{Xiong2021ApproximateNN} and improving DPR on noisy queries, \cite{Sidiropoulos2022AnalysingTR} \cite{Chen2022TowardsRD}