\documentclass[draftthesis,tocnosub,noragright,centerchapter,fullpagesingle,12pt]{uiuc_csthesis21}
\makeatletter
\usepackage{setspace} 
\usepackage[numbers, sort]{natbib}  
\usepackage{url}  
\usepackage{hyperref}
\usepackage{lscape} 
\def\fillandplacepagenumber{
	\par
	\pagestyle{empty}
	\vbox to 0pt{\vss}
	\vfill
	\vbox to 0pt{
		\baselineskip 0pt
		\hbox to \linewidth{\hss}
		\baselineskip\footskip
		\hbox to \linewidth{\hfil\thepage\hfil}\vss
	}
}
\usepackage{graphicx}
\usepackage{epsfig}   
\usepackage{caption}
\usepackage{makecell}
%\usepackage{algorithmic}
\usepackage{chronology}
\usepackage{subfigure}  % Useful for subfigures
\usepackage{booktabs}  
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage{algpseudocode}
%\usepackage{algorithm}
\usepackage{listings} 
\usepackage[ruled]{algorithm2e} 
\numberwithin{algocf}{chapter}
\usepackage{latexsym}
\usepackage{array}
\usepackage{booktabs}
\usepackage{pgfplots}
\usepackage{array}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{times}
\usepackage{inconsolata}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\newtheorem{remark}{Remark}[chapter]
\renewcommand{\qedsymbol}{QED.}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}
\phdthesis
\title{Efficient and Robust Web Scale Retrieval, Generation and Understanding}
\author{Daniel Campos}
\department{Computer Science}
\degreeyear{2023}
\advisor{Cheng Xiang Zhai }
\committee{Cheng Xiang Zhai, (UIUC, Chair)  \\
Alessandro Magnani (WalmartLabs, External Member) \\
Jiawei Han (UIUC)\\
Kevin Chang (UIUC)}
\begin{document}
\maketitle
\parindent 1em%

\frontmatter
\begin{abstract}
Large language models (LLM) effectively represent contextualized word representations across languages, domains, and tasks. Drive by these abilities; these models have become a building staple for many researchers and engineers who use text as their medium of representation, much like concrete is a staple in the construction world. Via the broad study and implementation, problems with large models have come to light: they can be expensive, brittle to noise, and produce unwanted outputs. Their large size and computational overhead make them difficult and expensive to deploy and use for inference. Minor variations in text inputs, such as typos or misspellings, can cause major losses in model accuracy. Seeking to improve how these models can be used for \textit{real world} usage and deployments, this thesis focuses on framing model used as a modular approach as model portions can be compressed, hardened, and optimized to deployment needs. To explore the challenges with large-scale deployments concerning robustness and inference efficiency, we explore four commonly used language workloads: textual understanding and classification, passage retrieval, text generation, and audio transcription generation. We chose these broad but connected tasks to ensure that our compression approaches broadly apply to natural language processing. First, we propose a general framework for improving model inference on broad language understanding workloads by studying how methods like unstructured pruning, structured pruning, and quantization can be leveraged to compress models and improve inference speeds. In studying compression for language understanding, we demonstrate that sparse language models can transfer to novel domains and tasks without further optimization. These sparse models can be combined with quantization and structured pruning to deliver massive speed-ups for minor losses in accuracy. Second, we study how leveraging multi-task modeling, knowledge distillation, and quantization can be leveraged to enable web-scale labeling of customer feedback. Using multi-task learning, task-specific knowledge, and model quantization, we can decrease the overall inference cost by 44\% for a major customer experience management company.  Third, we explore methods of tuning and optimizing dense retrieval methods post-training to ensure they perform well on real-world data. Our experiments yield simple and effective methods of increasing model robustness and decreasing inference costs without any need for retraining or index re-generation. Finally, we discuss our future work, which focuses on sequential compression approaches to sequence LLMs to allow generative workloads to reach web-scale deployments.
\end{abstract}
\tableofcontents
\mainmatter
\chapter{Introduction}
\label{chp:intro}
\input{introduction} 
\chapter{Literature Review}
\label{chp:lit}
\input{literature} 
\chapter{Introducing and Transferring Sparsity for Efficient Auto-Encoder Inference}
\label{chp:sparse}
\input{sparse}
\chapter{Robust and Efficient Semantic Retrieval}
\label{chp:search}
\input{search.tex}
\chapter{Scaling Multi-Lingual Classification and Abstractive Summarization to Web-Scale workloads}
\label{chp:Multi}
\input{scaling-MT-AS}
\bibliographystyle{IEEE_ECE}
\bibliography{custom}
\appendix
\label{chp:appendix}
\input{appendix.tex} 
\backmatter
\end{document}

