\section{Overview}
For the last decade, the common paradigm in using deep learning has been to improve model performance by improving architecture and scale. These models feature larger and larger, highly interconnected layers, otherwise referred to as dense. While this approach has continuously led to improvements in model performance, it is not without drawbacks. In 2011 a state-of-the-art computer vision model could run using a laptop. Now running a state-of-the-art language mode requires a cluster of specialized GPUs, which can cost upwards of \ 100,000 dollars and draw kilo-watts of power. \\
Inspired by the sparsity of the connections of neurons in the brain, unstructured sparsity seeks to improve the model efficiency by turning densely connected models into sparse models, which as a result, are far more efficient. While a large portion of existing research has focused on theory and high-level implementations, our work is focused on using the same sparsity to realize true measurable inference speedups. \\
In this chapter, we will discuss our broad experimentation focused on leveraging unstructured sparsity to improve efficiency for language model inference. First, our work examines \textit{The Optimal BERT Surgeon} where optimal zeroth and second-order pruning approaches are combined with quantization and structural pruning for a systematic approach for improving inference efficiency without using GPUs. Next, our work discusses \textit{Sparse*BERT} and broad experimentation on how sparse language models can be transferred to novel domains and tasks without further optimization. Finally, our work discusses OBERTa, which extends earlier experiments in sparse language modeling while improving training methods, model initialization, and distillation to deliver compelling inferences for overall text classification workloads. 
\section{The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models}
 
\section{Sparse*BERT: Sparse Models Generalize To New tasks and Domains}

\section{oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes}
\section{}
